# Chapter 10. Safety, Boundaries & Guardrails

**Prevent harmful outputs — allow only safe tasks**

---

#### 1. What do “safety and guardrails” mean?

When we use AI agents, we want them to be:

* **Helpful**
* **Safe**
* **Respectful**
* **Limited to allowed topics**

Just like a school teacher sets rules in a classroom, we must set **rules** for our AI agent:

* What it **can** do
* What it **cannot** do

This stops the agent from giving:

* Harmful advice
* Medical or legal instructions (if not allowed)
* Violent or abusive content
* Anything outside the boundaries you set

These rules are called **guardrails**.

---

#### 2. Real-life picture: A teacher supervising students

Imagine a teacher giving group work to students:

* Students can **discuss the assignment**
* Students can **ask questions**
* Students **cannot** abuse each other
* Students **cannot** wander into unrelated topics
* Students **cannot** do anything dangerous

The teacher steps in if they:

* Ask about something unsafe
* Try to do something harmful
* Break school rules

In the same way:

> An AI agent needs a “teacher-like” system of simple rules.

---

#### 3. Why guardrails matter in Autogen

Autogen agents are powerful.
They can:

* Write code
* Explain topics
* Plan tasks
* Give suggestions
* Talk to other agents

Without guardrails, they may:

* Respond to dangerous instructions
* Try to do tasks they shouldn’t
* Generate unwanted content

Guardrails ensure the agent only behaves inside a **safe zone**.

---

#### 4. Types of simple guardrails

Here are some common examples:

1. **Topic restrictions**

   * No violence
   * No personal attacks
   * No illegal advice

2. **Task restrictions**

   * Only answer math or science questions
   * Do not give medical prescriptions
   * Do not write malware

3. **Behavior restrictions**

   * Always reply politely
   * Avoid sensitive personal information
   * Redirect user if task is unsafe

You can enforce these with:

* Messages in `system_message`
* Manual checks in your Python code (simple `if-else`)
* Filters before sending messages to AI

---

#### 5. A simple guardrail idea (easy explanation)

Suppose you want the agent to:

* **Explain only simple school topics**
* Reject everything else

You can write simple logic like:

```
if message contains "violence" or "hack" or "illegal":
    respond with a safe warning
else:
    let the AI answer
```

This is not perfect, but for beginners it’s a **good start**.

---

#### 6. Simple Autogen example with guardrails (easy to understand)

Here’s a small script that checks the user’s message before letting the agent reply.

```python
from autogen import AssistantAgent

# 1. Create a safe assistant
safe_agent = AssistantAgent(
    name="safe_tutor",
    system_message=(
        "You are a very safe school tutor. "
        "You only help with simple subjects like math, science, and general learning. "
        "If the user asks anything harmful or illegal, refuse politely."
    ),
    llm_config={"model": "gpt-4o-mini", "temperature": 0.2}
)

# 2. Simple guardrail function
def check_safety(user_input):
    unsafe_keywords = ["violence", "kill", "weapon", "hack", "illegal", "harm"]
    
    for word in unsafe_keywords:
        if word in user_input.lower():
            return False  # unsafe
    return True  # safe

# 3. Main loop
while True:
    user_question = input("Enter your question (type 'exit' to quit): ")

    if user_question.lower() == "exit":
        break

    # Guardrail check
    if not check_safety(user_question):
        print("⚠️  This topic is not allowed. Please ask a safe, school-related question.\n")
        continue

    # Safe → let Autogen answer
    reply = safe_agent.generate_reply(messages=[{"role": "user", "content": user_question}])
    print("AI:", reply, "\n")
```

**What this script does:**

1. It checks the user’s question.
2. If it finds unsafe words → stops the agent from answering.
3. If safe → lets the agent respond normally.

This is a **basic but very useful** starting point.

---

#### 7. How this helps in real apps

These guardrails can be used in:

* **Learning apps** (avoid harmful content)
* **Student homework helpers**
* **Clinic assistants** (don’t allow diagnosis/prescriptions)
* **Business bots** (don’t expose private data)
* **Customer support systems**
* **Kids-friendly AI apps**

Even simple rules make the system much safer.

---

#### 8. Takeaway

* Guardrails protect users and keep AI focused on **allowed tasks**.
* Simple `if-else` filters can block obvious harmful topics.
* System messages can reinforce “allowed boundaries.”
* Human supervision + guardrails = safe and reliable AI.

This chapter gives you a **foundation** for responsible agent design.

---

### Codex task for this chapter

**Instruction for Codex:**

> “Add safety rules to an Autogen agent using simple if-else checks. Block messages containing unsafe keywords (like ‘violence’, ‘hack’, ‘illegal’) and reply with a polite warning. Allow all other questions to be answered normally. Add clear comments explaining each part.”

# **Chapter 29 — Cost Control & Optimization**

### **Short Introduction**

Using AI models is like using electricity at home — very powerful, but you must watch the bill.
When building agentic systems, **every message uses tokens**, and tokens cost money.
This chapter helps you understand how to keep AI usage efficient, predictable, and affordable.

---

## **Key Ideas Explained Step by Step**

### **1. What are tokens (simple explanation)**

Think of tokens as **pieces of text**.

For example:

* “Apple” → 1 token
* “I want to buy an apple.” → around 7–8 tokens

Every message you send to the model and every reply the model gives is counted in tokens.

More text → more tokens → more cost.

---

### **2. Why token limits matter**

Imagine a phone plan with a data cap:

* Light use = cheap
* Heavy use = expensive

AI is similar.
If your agent sends very long prompts or too many messages, the cost increases quickly.

So you need to:

* Keep prompts short
* Reuse memory smartly
* Avoid unnecessary explanations
* Choose the right model size
* Monitor usage

---

### **3. Choosing the right model**

Not every task needs the biggest model.

Use the same logic as buying the right-sized appliance:

* For boiling water → a small kettle
* For cooking a feast → a big stove

Similarly:

| Task Type          | Best Model           | Why                   |
| ------------------ | -------------------- | --------------------- |
| Simple tasks       | Small/cheap model    | Fast & cost-effective |
| Medium tasks       | Mid-size             | Good balance          |
| Complex reasoning  | Larger model         | Needs deeper thinking |
| Coding-heavy tasks | Model tuned for code | More accurate         |

Right model = lower cost + faster results.

---

### **4. How to optimise prompts**

Here are simple techniques:

* Remove unnecessary words
* Use clear instructions
* Break tasks into steps (smaller prompts)
* Summarise long histories
* Store context in vector memory instead of sending entire conversations each time

Good prompts = fewer tokens.

---

### **5. Practical Everyday Example**

Suppose you run an AI assistant for a school.

If the student asks:

“Explain the photosynthesis process in 500 words with examples and short notes.”

A large model may write too much and cost more.

Instead:

* Use a medium model
* Set a limit like: “Explain in 150 words.”
* Or “Explain in bullet points only.”

This reduces cost without losing quality.

---

### **6. Cost control during agent conversations**

When multiple agents talk to each other, tokens multiply quickly because:

* Agent A message = cost
* Agent B reply = cost
* Additional memory retrieval = cost

So it’s important to **track usage**.

Autogen can easily print:

* tokens used per message
* total tokens used in a session
* estimated cost

This helps you monitor and adjust.

---

## **Takeaway Summary**

* Tokens = pieces of text that cost money.
* Choose model size based on task complexity.
* Keep prompts clean and short.
* Use memory to avoid sending giant histories.
* Track token usage during agent conversations.
* Good optimisation = high performance + low cost.

---

## **Codex Instruction Task**

**“Add a token-usage printout in an Autogen conversation.
After each agent message, print:

* tokens used,
* cumulative tokens,
* estimated cost.
  Include simple comments so beginners understand how the tracking works.”**

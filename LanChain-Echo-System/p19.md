# 19. Multi-Agent Collaboration

## 1. Introduction

So far you can imagine **an agent** as:

- a smart assistant,
- that can **think** (use the model),
- and **act** (call tools, chains, or other code).

Multi-agent collaboration is what happens when you have **a small team of agents**:

- each agent has a **role**,
- they **talk to each other** through messages or shared state,
- and together they solve a task that would be messy for a single “do-everything” agent.

In backend terms, multi-agent systems feel a lot like:

- microservices with their own responsibilities,
- orchestrated by a controller or workflow engine,
- using shared data stores or message queues to communicate.

You’ll see how LangChain and Codex help you design:

- a **coordinator agent**,
- several **specialist agents**,
- simple message passing between them,
- and a clean way to grow this “team” over time.

---

## 2. Why Backend Developers Should Care

As a backend developer you already:

- split your system into **services** (auth, billing, reporting),
- use **workers** to process jobs,
- have APIs that **call each other**,
- and use **queues** or **databases** to share data.

Multi-agent collaboration maps nicely to this world:

- each agent can act like a **service** with a clear API,
- a coordinator can act like a **workflow engine** or **API gateway**,
- tools look like **helper libraries or microservices**,
- messages and state look like **JSON payloads** or **rows in a DB**.

This pattern matters when:

- one agent prompt becomes **too big and confusing**,
- you want **clear separation of responsibilities** (support vs billing vs analytics),
- you need **different safety rules** per task (e.g. finance vs general chat),
- you want to **scale and evolve** the system gradually.

Once you see multi-agent systems as **“LLM-powered microservices”**, it becomes much easier to design, test, and deploy them.

---

## 3. Real-Life Analogy

Imagine you’re **renovating a house**:

- you hire a **general contractor**,
- plus specialists:
  - electrician,
  - plumber,
  - painter,
  - carpenter.

The contractor:

- talks to you (the “user”),
- breaks your request into **tasks**,
- decides **who should do what**,
- coordinates **order and timing**,
- and updates you on **progress**.

Each specialist:

- focuses on **their own job**,
- brings their **own tools**,
- doesn’t need to understand the whole project,
- just receives clear instructions like:
  - “Install these lights in the kitchen,”
  - “Fix this pipe in the bathroom.”

Multi-agent collaboration is the same:

- the **coordinator agent** = general contractor,
- **worker agents** = specialists with their own tools and prompts,
- **messages/state** = task descriptions, notes, and project board.

---

## 4. Key Concepts

Here are the main ideas in multi-agent collaboration, translated into backend language:

- **Role**
  - What this agent is responsible for.
  - Example: `support_agent`, `billing_agent`, `analytics_agent`.

- **Coordinator Agent**
  - Receives the **original user request**.
  - Decides **which agents** should help.
  - Orchestrates the order of calls and combines results.
  - Similar to a **workflow engine** or **API gateway**.

- **Worker / Specialist Agent**
  - Focused on a single domain.
  - Has its own **tools**, **prompt**, and sometimes its own **memory**.
  - Similar to a **microservice**.

- **Communication Channel**
  - How agents share information.
  - Could be:
    - direct function calls (`agent.invoke()`),
    - shared in-memory state (Python dict),
    - or stored in an external system (DB, cache, message bus).

- **Shared State**
  - A structured object (often a dict) that holds:
    - user request,
    - intermediate results,
    - decisions already made.
  - Similar to a **workflow context** or **job payload**.

- **Collaboration Pattern**
  - How agents work together:
    - **Sequential**: one after another.
    - **Parallel**: multiple workers at the same time.
    - **Conversation**: agents talk back and forth.
  - In more advanced setups, LangGraph models these as **nodes and edges**.

---

## 5. Steps Explained Simply

Here is a simple way to design a multi-agent system as a backend developer.

### Step 1: Define the Overall Job

Start from the **user’s viewpoint**:

- “I want an AI assistant that:
  - reads a customer complaint,
  - checks billing history,
  - proposes a refund,
  - and drafts a reply email.”

This is your **high-level workflow**.

### Step 2: Identify Specialist Roles

Split the job into **clear roles**:

- `support_agent`
  - understands the complaint,
  - summarizes it in plain language.
- `billing_agent`
  - checks invoices, payments, and refunds,
  - suggests a fair refund / action.
- `writer_agent`
  - drafts a friendly, human-like reply email.

Each role should feel like a **small, single-purpose microservice**.

### Step 3: Decide How They Collaborate

For the example above, a simple **sequential pattern** works:

1. Coordinator sends the complaint to `support_agent`.
2. Coordinator sends user + summary to `billing_agent`.
3. Coordinator sends everything to `writer_agent`.
4. Coordinator returns the final email to the user.

You could later:

- make some calls **parallel** (e.g. analytics + support),
- or allow **back-and-forth** (e.g. writer asks billing for clarification).

### Step 4: Design the Shared State

Create a simple Python dict that holds everything:

```python
state = {
    "user_input": "...",
    "ticket_summary": "",
    "billing_decision": "",
    "final_reply": "",
}
```

Each agent:

- **reads** what it needs from `state`,
- **writes** its output back to `state`.

This is very similar to:

- a job payload in a task queue,
- or a JSON object passed through a pipeline of functions.

### Step 5: Implement Specialists as Normal Agents

For each role:

- create tools that make sense for that domain,
- write a **focused system prompt**,
- wrap it in a LangChain agent (`AgentExecutor`).

You do **not** need special “multi-agent” APIs:

- you just have **multiple normal agents**,
- plus a Python function that coordinates them.

### Step 6: Implement the Coordinator

Finally, write a **coordinator function** that:

- initializes `state`,
- calls each specialist agent in the right order,
- passes relevant parts of `state` as `input`,
- collects outputs back into `state`,
- returns the final answer to the caller.

Later, if flows get more complex, you can move this logic into **LangGraph** for better control and visualization.

---

## 6. Short Code Examples

These examples are intentionally small and simplified.  
They focus on the **pattern**, not production concerns.

### Example 1: Two Simple Specialist Agents

```python
# agents/support_agent.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.agents import AgentExecutor, create_tool_calling_agent


def create_support_agent() -> AgentExecutor:
    llm = ChatOpenAI(model="gpt-4.1-mini")

    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "You are a support analyst.\n"
         "Summarize customer issues in 2-3 sentences,\n"
         "focusing on what went wrong and any urgency."),
        ("user", "{input}"),
    ])

    tools = []  # no tools yet; pure reasoning
    agent = create_tool_calling_agent(llm, tools, prompt)
    return AgentExecutor(agent=agent, tools=tools)
```

```python
# agents/billing_agent.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.agents import AgentExecutor, create_tool_calling_agent

from tools.billing_tools import get_billing_history  # your own tool


def create_billing_agent() -> AgentExecutor:
    llm = ChatOpenAI(model="gpt-4.1-mini")

    tools = [get_billing_history]

    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "You are a billing specialist.\n"
         "Given a customer summary and ID, review their history\n"
         "and propose a fair resolution (refund, credit, or none)."),
        ("user", "{input}"),
    ])

    agent = create_tool_calling_agent(llm, tools, prompt)
    return AgentExecutor(agent=agent, tools=tools)
```

Each agent:

- has a **single responsibility**,
- has its own **prompt** and **tools**,
- can be tested independently.

### Example 2: Simple Coordinator Function

```python
# app/multi_agent_coordinator.py
from agents.support_agent import create_support_agent
from agents.billing_agent import create_billing_agent


def handle_customer_issue(user_input: str, customer_id: str) -> str:
    state = {
        "user_input": user_input,
        "customer_id": customer_id,
        "ticket_summary": "",
        "billing_decision": "",
    }

    support = create_support_agent()
    billing = create_billing_agent()

    # 1) Support agent summarizes the issue
    support_result = support.invoke({"input": state["user_input"]})
    state["ticket_summary"] = support_result.get("output", "")

    # 2) Billing agent decides on a resolution
    billing_prompt = (
        f"Customer ID: {state['customer_id']}\n\n"
        f"Ticket summary: {state['ticket_summary']}\n\n"
        "Propose a billing resolution."
    )
    billing_result = billing.invoke({"input": billing_prompt})
    state["billing_decision"] = billing_result.get("output", "")

    # 3) Final combined message (you could add a writer agent here)
    final_reply = (
        "Issue summary:\n"
        f"{state['ticket_summary']}\n\n"
        "Proposed billing resolution:\n"
        f"{state['billing_decision']}"
    )
    return final_reply
```

This coordinator:

- acts like a **controller**,
- calls specialist agents in sequence,
- passes context via a simple dict,
- returns a clear, combined result.

You can later:

- swap `handle_customer_issue` with a **LangGraph workflow**,
- add more agents (e.g. `writer_agent`, `compliance_agent`),
- or run some agents **in parallel** when safe.

---

## 7. How Codex Helps

Codex is very good at the **plumbing work** of multi-agent systems so you can focus on design and safety.

You can ask Codex to:

- **Split a large agent into specialists**
  - “Take this big support agent and split it into `support_agent`, `billing_agent`, and `writer_agent` with separate prompts and tools.”

- **Generate coordinator code**
  - “Create a coordinator function that calls support → billing → writer agents, passing a shared state dict.”

- **Add tools per agent**
  - “For the billing agent, create tools for `get_billing_history` and `create_refund`, with clear type hints.”

- **Introduce tests**
  - “Write pytest tests that mock the LLM and tools, and assert that the coordinator calls each agent in the right order.”

- **Refactor to LangGraph**
  - “Turn this coordinator into a LangGraph workflow where each agent is a node and the shared state is passed along.”

Codex can read your existing code, detect patterns, and propose patches that:

- respect your folder structure,
- keep responsibilities separate,
- and gradually move you from a single agent to a **clean multi-agent architecture**.

---

## 8. Small Diagram

### High-Level Multi-Agent Collaboration

```text
User Request
    ↓
[Coordinator Agent]
    ↓
 ┌───────────────┐      ┌───────────────┐
 │ Support Agent │      │ Billing Agent │
 │ (summarize)   │      │ (decide)      │
 └───────┬───────┘      └───────┬───────┘
         │                      │
         └───────────┬──────────┘
                     ↓
              [Combined Result]
                     ↓
                Final Reply
```

### Shared State Idea

```text
state = {
  "user_input": ...,
  "ticket_summary": ...,
  "billing_decision": ...,
}

Support Agent  → fills ticket_summary
Billing Agent  → reads summary, fills billing_decision
Coordinator    → reads both, builds final answer
```

---

## 9. Summary

- Multi-agent collaboration means using **a team of agents**, each with a **clear role**, instead of one giant “do everything” agent.
- A **coordinator agent** acts like a workflow engine, routing work to **specialist agents** and combining their results.
- Communication happens through **function calls**, **messages**, or a shared **state object**, just like job payloads or workflow context in backend systems.
- You can start with a simple **sequential coordinator function**, then grow into **parallel** flows or **LangGraph workflows** as things get more complex.
- Codex helps by **splitting large agents into specialists**, generating **coordinator code and tools**, adding **tests**, and refactoring everything into a clean, maintainable multi-agent architecture.


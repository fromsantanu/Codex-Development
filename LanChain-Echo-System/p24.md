# 24. Adding Memory and State

## 1. Introduction

Most simple LLM examples are “one‑shot”: you send a prompt, get a response, and forget everything.  
Real applications are different. Users come back, ask follow‑up questions, and expect the system to remember what happened before.

In this chapter we look at **memory** and **state**:
- Memory = what the system remembers across turns.
- State = the structured data flowing through your workflow right now.

You’ll see how to design both so your agentic workflows feel consistent and reliable, not random.

---

## 2. Why backend developers should care

- Users expect conversations to feel like a logged‑in session, not a stateless `POST /ask` every time.
- Business rules often depend on history:
  - “If user already confirmed, skip this step.”
  - “If we recommended this last time, try something different.”
- You already manage state in APIs (sessions, carts, tokens); this is the same idea, just with LLMs in the loop.
- Good state design makes your graphs testable:
  - You can create a fake state and see how the workflow behaves.
- Storing memory let you:
  - Personalize answers.
  - Avoid re‑asking questions.
  - Build richer, long‑running flows.

---

## 3. Real-life analogy

Imagine working with a **personal assistant**:

- **Memory** is their notebook:
  - Your preferences (no meetings before 10am).
  - Past decisions (we chose supplier B last month).
  - Ongoing tasks (follow up with client X).

- **State** is the whiteboard for the **current task**:
  - Today’s agenda.
  - The current email they’re drafting.
  - The information they collected for *this* request.

The notebook stays when the task ends; the whiteboard is wiped and redrawn for each new task.

In agentic systems:
- Memory is what survives across calls/sessions.
- State is the structured object flowing through your graph during one run.

---

## 4. Key concepts

- **Stateless vs stateful LLM calls**  
  - Stateless: each request is independent; no history.
  - Stateful: answer depends on previous messages, tool calls, or stored data.

- **Conversation history**  
  - A log of messages (user + assistant) that is fed back into the model so it can answer follow‑up questions.

- **Short‑term memory**  
  - What the model sees in the current context window:
    - Recent conversation turns.
    - Important notes extracted from earlier.

- **Long‑term memory**  
  - Information stored outside the LLM:
    - Database rows.
    - Vector store of user documents.
    - Key‑value store with per‑user preferences.

- **State object (LangGraph / LangChain)**  
  - A structured dictionary that moves through the workflow:
    - `input`, `messages`, `tools_used`, `result`, `user_id`, etc.
  - Each node/function reads some fields and writes others.

- **Session identifier**  
  - A key that ties requests together:
    - `session_id`, `user_id`, or `conversation_id`.
  - Used to load and save memory between runs.

- **Persistence layer**  
  - Where you store memory and state snapshots:
    - SQL, Redis, S3, vector DB, etc.

---

## 5. Steps explained simply

1. **Decide what you actually need to remember**  
   - User identity?
   - Conversation history?
   - Preferences (language, tone)?
   - Task‑specific data (chosen plan, confirmed details)?

2. **Design a state model**  
   - Create a typed dictionary / Pydantic model for graph state:
     - `input`: current user message.
     - `history`: list of past messages or summaries.
     - `facts`: key information the model extracted.
     - `answer`: final response.

3. **Add in‑graph memory (short‑term)**  
   - Each time the user talks:
     - Load existing history for that session from storage.
     - Pass it into the graph as part of the initial state.
     - Append the new user message and assistant reply to `history`.

4. **Add long‑term memory where needed**  
   - For important facts that should live beyond a few turns:
     - Save them into a DB or vector store.
     - Create tools that let the agent query this memory (`search_user_notes`, `load_profile`, etc.).

5. **Control what you send back to the LLM**  
   - Avoid stuffing the whole history every time.
   - Use strategies like:
     - Keep last N messages.
     - Summarize older messages into short notes.
     - Only include relevant facts for the current request.

6. **Wire memory into your backend**  
   - Use `session_id` or `user_id` to:
     - Load previous state and history before running the graph.
     - Save updated state and history after the graph finishes.

7. **Test and monitor**  
   - Simulate multiple turns in tests:
     - “User: set my name to Alice”
     - “User: what’s my name?”
   - Check that:
     - Memory is loaded and saved correctly.
     - Old sessions don’t leak into new ones.

---

## 6. Short code examples

Below is a small example that:
- Keeps a simple text history as memory.
- Maintains a structured state object in LangGraph.
- Shows how a backend could load and save memory per session.

```python
from typing import TypedDict, List, Dict
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END


# 1) Define the state for one run of the graph
class ChatState(TypedDict):
    user_id: str
    input: str
    history: List[str]  # short-term memory for this user
    answer: str


llm = ChatOpenAI(model="gpt-4.1-mini")


# 2) Simple in-memory store (replace with DB/Redis in real code)
USER_HISTORY: Dict[str, List[str]] = {}


def load_history(user_id: str) -> List[str]:
    return USER_HISTORY.get(user_id, [])


def save_history(user_id: str, history: List[str]) -> None:
    USER_HISTORY[user_id] = history


# 3) Node that uses history + input to generate an answer
def chat_node(state: ChatState) -> ChatState:
    # Combine history and new message into one prompt
    history_text = "\n".join(state["history"])
    prompt = (
        "You are a helpful assistant.\n"
        "Conversation so far:\n"
        f"{history_text}\n\n"
        f"User: {state['input']}\n"
        "Answer briefly and remember important details."
    )

    result = llm.invoke(prompt)
    reply = str(result.content)

    # Update history with this turn
    new_history = state["history"] + [f"User: {state['input']}", f"Assistant: {reply}"]

    return {
        **state,
        "history": new_history,
        "answer": reply,
    }


# 4) Build the graph
workflow = StateGraph(ChatState)
workflow.add_node("chat", chat_node)
workflow.set_entry_point("chat")
workflow.add_edge("chat", END)

app = workflow.compile()


# 5) Example backend helper
def handle_message(user_id: str, message: str) -> str:
    # Load memory for this user
    history = load_history(user_id)

    # Run the graph with current state
    state = app.invoke(
        {
            "user_id": user_id,
            "input": message,
            "history": history,
        }
    )

    # Save updated memory
    save_history(user_id, state["history"])

    return state["answer"]


# In your actual HTTP handler you would call:
#   reply = handle_message(user_id="u123", message="What's the status of my project?")
```

Key ideas:
- `ChatState` is the **state** for this run.
- `history` is the **short‑term memory** that we:
  - Load before the graph.
  - Update inside the graph.
  - Persist after the graph.

You can extend this pattern with:
- Separate tools to read/write long‑term memory (DB, vector store).
- Fields like `facts` or `profile` in the state to hold extracted information.

---

## 7. How Codex helps

When you add memory and state, Codex can:

- Propose a clean state model (`ChatState`, `OrderState`, etc.) based on your business needs.
- Generate code to:
  - Load and save memory from SQL, Redis, or a vector DB.
  - Wrap your storage as tools (`save_user_pref`, `get_user_notes`).
- Suggest strategies to limit context size (summarization, last‑N messages, relevance filtering).
- Refactor ad‑hoc “history” handling into:
  - A clear memory layer.
  - Reusable helper functions.
  - Properly typed fields in your state.
- Generate tests that:
  - Simulate multi‑turn conversations.
  - Assert that memory behaves correctly (e.g. remembers name, preferences).

This reduces the risk of subtle bugs like “user A sees user B’s history” or “model forgets important info after a few turns”.

---

## 8. Small diagram

State and memory flow:

```text
        ┌───────────────────┐
User →  │ HTTP API / Backend│
        └─────────┬─────────┘
                  │  load history (by user_id)
                  ↓
           ┌─────────────┐
           │ LangGraph   │
           │  (Chat App) │
           └─────┬───────┘
                 │  uses state:
                 │  {user_id, input, history, answer}
                 ↓
           [chat node / agent]
                 ↓
           updated state + history
                 │
                 ↓
        save history (by user_id)
                 │
                 ↓
             API response
```

Another way to see it:

```text
[Load memory] → [Run graph with state] → [Update memory] → [Reply]
```

---

## 9. Summary

- Memory is what your system remembers across requests; state is the structured data for the current run.
- You already manage similar concepts in backend work (sessions, carts, profiles); now you’re applying them to LLM‑driven workflows.
- A well‑designed state object and memory layer make your agentic systems predictable, testable, and personalized.
- LangGraph lets you pass state through nodes, while your own storage (SQL, Redis, vector DB) holds long‑term memory.
- Codex can help design the state models, storage integration, and tests so you can focus on what should be remembered, not just how.


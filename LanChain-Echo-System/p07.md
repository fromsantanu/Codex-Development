# 7. Installing LangChain + LangGraph

Before we can build any smart agents or graphs, we need to **install the right Python libraries** and **wire them into a clean environment**.

You can think of this chapter as:

> “Setting up a fresh Python project with a few extra libraries (LangChain + LangGraph) so you can treat AI workflows just like any other backend service.”

We’ll keep everything in **normal backend terms**: virtual environments, `pip install`, `.env` files, and a tiny smoke test.

---

## 1. Simple introduction

At a high level, you will:

- create a **Python virtual environment** for your agent project,
- install **LangChain**, **LangGraph**, and a few helper packages,
- create a **`.env` file** for API keys (like `OPENAI_API_KEY`),
- run a **small test script** to confirm everything works.

Nothing special or “AI‑only” here — it’s the same pattern you already use when:

- setting up a new FastAPI service,
- adding SQLAlchemy,
- and configuring environment variables.

LangChain and LangGraph are **just libraries** in your stack.

---

## 2. Why it matters for backend developers

You could technically `pip install` everything globally and start coding, but that leads to:

- version conflicts between projects,
- “it works on my machine” problems,
- hard‑coded API keys sneaking into code.

By doing a small amount of setup work:

- each agent project gets its **own isolated environment** (`venv`),
- dependencies are **pinned and repeatable**,
- secrets stay in **environment variables**, not in source code,
- teammates (and CI) can **reproduce your setup** easily.

In other words, you treat AI libraries **exactly like**:

- `fastapi`, `sqlalchemy`, `alembic`,
- or any other production dependency.

That’s the mindset: **AI code is still normal backend code**, so we give it the same discipline.

---

## 3. Real-life analogy

Imagine you’re setting up a new **workbench in your garage**:

- You don’t throw all tools into one giant box.
- You create a **separate toolbox** for a specific job:
  - “This box is for electrical work.”
  - “This one is for plumbing.”

For each toolbox you:

- add **only the tools you need**,
- label it clearly,
- keep a small **checklist** of what’s inside.

In this analogy:

- the **virtual environment** is your dedicated toolbox,
- `pip install` is how you **add tools** into it,
- the **`.env` file** is a **note taped inside the lid** telling you:
  - “Here are the secret codes (API keys) this toolbox uses.”

LangChain and LangGraph are simply **new power tools** you’re adding into one well‑organized box.

---

## 4. Key concepts

Here are the main ideas, mapped to things you already know:

- **Virtual environment (`venv`)**
  - A **per‑project Python environment**.
  - Like a local “container” for dependencies.

- **Package installation (`pip install`)**
  - You’ll install:
    - `langchain` – building blocks (LLMs, tools, chains, agents),
    - `langgraph` – graph/workflow engine,
    - `langchain-openai` – connects LangChain to OpenAI models,
    - `python-dotenv` – loads `.env` files into environment variables.

- **Environment variables + `.env` file**
  - You never hard‑code secrets into Python files.
  - Instead, you store them in `.env` and load them at runtime.
  - Example: `OPENAI_API_KEY`, possibly `LANGCHAIN_TRACING_V2`, etc.

- **Smoke test script**
  - A tiny Python file that:
    - loads env vars,
    - calls a model via LangChain,
    - and maybe constructs a tiny LangGraph.
  - If this works, your install is healthy.

Once these are in place, you can start building:

- tools, chains, agents (LangChain),
- and graph‑based workflows (LangGraph),
without fighting environment issues.

---

## 5. Step-by-step explanation

### Step 1: Create a new project folder

Pick or create a folder for your agent project:

```bash
mkdir my-agentic-app
cd my-agentic-app
```

You can of course use any existing repo; this is just the cleanest way to start.

---

### Step 2: Create and activate a virtual environment

Create a `.venv` folder. Use the command that matches your OS.

**Mac / Linux:**

```bash
python -m venv .venv
source .venv/bin/activate
```

or, if `python` points to Python 2 on your system:

```bash
python3 -m venv .venv
source .venv/bin/activate
```

**Windows (PowerShell or cmd):**

```bash
py -m venv .venv
.venv\Scripts\activate
```

After activation, your shell prompt usually shows `(.venv)` at the beginning.  
Anything you `pip install` now will live **inside this project only**.

---

### Step 3: Install LangChain, LangGraph, and helpers

With the virtual environment active, upgrade `pip` and install the libraries:

```bash
pip install --upgrade pip
pip install langchain langgraph langchain-openai python-dotenv
```

Optionally, you might later add:

- database drivers (e.g., `psycopg2-binary`, `sqlalchemy`),
- web frameworks (e.g., `fastapi`, `uvicorn`),
- vector DB clients, etc.

But the core AI pieces for this book are:

- `langchain`,
- `langgraph`,
- `langchain-openai`,
- `python-dotenv`.

---

### Step 4: Create a `.env` file for secrets

In the project root (same folder as `.venv`), create a file named `.env`:

```text
OPENAI_API_KEY=sk-your-real-key-goes-here
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=my-first-agent
```

Notes:

- Never commit the **real** `.env` file to git.
- Instead, commit a **`.env.example`** with placeholder values.
- Treat the real `.env` like any other secret configuration.

This is exactly the same pattern you use for:

- database passwords,
- JWT secrets,
- API tokens for third‑party services.

---

### Step 5: Load environment variables in Python

Create a small file like `config.py`:

```python
from dotenv import load_dotenv
import os

# Load variables from .env into os.environ
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY is not set. Check your .env file.")
```

This gives you:

- a **single place** to handle configuration,
- a clear error if the key is missing,
- a pattern you can reuse later (DB URLs, tracing flags, etc.).

You can import `OPENAI_API_KEY` anywhere in your project,  
or just import `config` to ensure `.env` is loaded once.

---

### Step 6: Run a tiny LangChain smoke test

Now create `test_langchain.py` to verify the model call works:

```python
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

load_dotenv()  # load OPENAI_API_KEY from .env

llm = ChatOpenAI(model="gpt-4.1")  # or any available chat model

response = llm.invoke("Say hello in one short sentence.")
print(response.content)
```

Run it:

```bash
python test_langchain.py
```

If everything is set up correctly, you should see a short greeting printed out.  
That confirms:

- `.env` is loading,
- the API key works,
- LangChain + `langchain-openai` are installed correctly.

---

### Step 7: Run a tiny LangGraph smoke test

To ensure LangGraph is installed and wired correctly,  
create `test_langgraph.py`:

```python
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph

load_dotenv()

llm = ChatOpenAI(model="gpt-4.1")

def greet(state: dict) -> dict:
    """Simple node function that calls the model."""
    question = state.get("question", "Say hello.")
    answer = llm.invoke(question)
    return {"question": question, "answer": answer.content}

# Build a minimal graph with a single node
graph = StateGraph()
graph.add_node("greet", greet)

# For now we just use the compiled graph as a callable
workflow = graph.compile()

result = workflow.invoke({"question": "Explain in one sentence what this script does."})
print(result["answer"])
```

Run it:

```bash
python test_langgraph.py
```

If you see a one‑sentence explanation, you know:

- `langgraph` is installed,
- it can call LangChain models,
- and basic graph wiring works.

We’ll build **much richer graphs** later; this is just a **sanity check**.

---

## 6. Short code examples

Here are the two most important snippets from above, in compact form.

### Example 1: Installing packages (shell)

```bash
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

pip install --upgrade pip
pip install langchain langgraph langchain-openai python-dotenv
```

### Example 2: Minimal LangChain test (Python)

```python
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

load_dotenv()
llm = ChatOpenAI(model="gpt-4.1")
print(llm.invoke("Hello!").content)
```

These two are usually enough to prove your environment is correct.

---

## 7. How Codex helps

Codex can remove a lot of the “remember every command” burden.  
Here are concrete ways it can help in this chapter:

- **Creating the environment setup**
  - Prompt idea:
    - “In this empty folder, create a Python project for LangChain + LangGraph with a `.venv` and `requirements.txt`. Include `langchain`, `langgraph`, `langchain-openai`, and `python-dotenv`.”

- **Generating config and example files**
  - Codex can create:
    - `config.py` that loads `.env`,
    - `test_langchain.py` and `test_langgraph.py`,
    - `.env.example` with placeholder keys.

- **Documenting the setup**
  - Prompt idea:
    - “Write a `README.md` section that explains how to set up the virtual environment, install LangChain + LangGraph, and run a smoke test.”

- **Keeping things consistent over time**
  - When you later add more libraries (FastAPI, SQLAlchemy, vector DBs), you can ask:
    - “Update requirements and config files to include these new dependencies and show how they fit into the existing LangChain setup.”

In other words, you handle the **intent and architecture**,  
and Codex takes care of generating the **boring setup code and docs**.

---

## 8. Diagrams

### High-level environment setup

```text
[Create Project Folder]
        ↓
[Create .venv] → [Activate .venv]
        ↓
[pip install LangChain + LangGraph]
        ↓
[Add .env with OPENAI_API_KEY]
        ↓
[Run smoke tests]
```

### How config flows into your code

```text
.env file
   ↓ (load_dotenv)
Environment variables (OPENAI_API_KEY, etc.)
   ↓
LangChain / LangGraph code
   ↓
Calls to OpenAI and other services
```

Once this pipeline works, building agents and graphs becomes just:

```text
[Write tools/agents/graphs] → [Run app/tests]
```

---

## 9. Summary

- Installing **LangChain** and **LangGraph** is standard Python work:
  - create a virtual environment,
  - `pip install` the right packages,
  - and load secrets from a `.env` file.
- A small **smoke test** with LangChain and LangGraph gives you confidence that your environment and API keys are wired correctly.
- Treat these libraries like any other backend dependency:
  - version‑controlled requirements,
  - clean config management,
  - repeatable setup instructions for teammates and CI.
- Codex can:
  - generate the environment setup files,
  - create test scripts,
  - and document the steps for you,  
  so you can focus on designing agents, tools, and workflows instead of typing boilerplate.


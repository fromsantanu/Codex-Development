# 3. What Is LangChain?

LangChain might sound like a new, mysterious framework, but you can think of it very simply as:

> “A Python library that helps you **organize AI calls** (models, tools, memory, workflows) like you already organize normal backend logic (services, helpers, pipelines).”

Instead of calling an AI model directly everywhere, LangChain gives you:

- clear building blocks (LLMs, tools, prompts, chains, agents),
- ways to **connect** them,
- and patterns to **reuse** them across your app.

---

## 1. Simple introduction

Without LangChain, a typical “AI script” often looks like:

- import the model,
- write a prompt as a string,
- call the model,
- manually parse the response,
- manually call more functions,
- repeat.

This quickly becomes one big messy script.

With LangChain, you:

- wrap models, prompts, and functions into **components**,
- connect them into **chains** (pipelines),
- optionally add an **agent** on top that can pick tools automatically.

So LangChain is:

- not a model itself,
- not a database,
- but a **framework** to **structure AI-related code** in a clean, backend‑friendly way.

---

## 2. Why it matters for backend developers

As a backend engineer, you care about:

- clarity of flow,
- testability,
- reusability,
- separation of concerns.

Raw AI calls (just “call the model with a big string”) don’t give you that.  
They are hard to test, hard to reuse, and easy to break.

LangChain helps by:

- giving you **standard patterns**:
  - `Chains` = pipelines of steps,
  - `Tools` = wrapped functions/services,
  - `Agents` = decision-makers using tools,
  - `Memory` = state across turns.
- making it easier to:
  - mock parts in tests,
  - reuse prompts and components,
  - plug into other tools like LangGraph, LangServe.

In other words:

> LangChain turns “one-off AI scripts” into **real maintainable backend components**.

---

## 3. Real-life analogy

Think of a **package delivery network**:

- You have **hubs** (sorting centers),
- **routes** (how packages move),
- **rules** (which hub handles what),
- and **workers** who move packages.

Without structure:

- Everyone just passes boxes around randomly — chaos.

With structure:

- Packages move through **well‑defined paths**:
  - start → hub A → hub B → destination.

In this analogy:

- The **user request** is the package,
- **Chain** is the route (step‑by‑step path),
- **LLM + tools** are the workers at each hub,
- **LangChain** is the system that defines:
  - what hubs exist,
  - what each hub can do,
  - in which order they are visited.

So LangChain is like your **routing system** for AI‑related work.

---

## 4. Key concepts

Here are the main LangChain ideas, mapped to traditional backend concepts:

- **LLM (Large Language Model)**
  - The AI engine that reads text and generates text.
  - Think of it as a **very advanced autocomplete**.

- **Prompt**
  - The instructions you send to the model.
  - Like a **Jira ticket** or **spec** that tells the model what to do.

- **Chain**
  - A **pipeline of steps**, such as:
    - build prompt → call model → parse result → call another function.
  - Similar to:
    - multiple stored procedures or services called in a set order.

- **Tool**
  - A wrapped function or service the model is allowed to call.
  - Very similar to an application service method or microservice endpoint.

- **Agent**
  - A model (LLM) that can look at a list of tools and **decide**:
    - which tool to use next,
    - with what arguments,
    - and when to stop.
  - Think of it as a **smart controller**.

- **Memory**
  - Where conversation history or state is kept between calls.
  - Similar to:
    - session state,
    - cached context,
    - or a conversation log in a DB.

LangChain gives you classes and utilities for all of these, so you don’t reinvent them for every new feature.

---

## 5. Step-by-step explanation

Let’s build up from a simple chain to an agent, step by step.

### Step 1: Start with a plain model

You configure a model using `langchain-openai`:

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4.1-mini")
```

Now `llm` can take a prompt and return an answer.

### Step 2: Add a prompt template

Instead of writing the full text each time, you define a template:

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a friendly tutor for backend developers."),
    ("user", "{question}")
])
```

This is like having a **parameterized spec**:

- you set the role once,
- and fill in `{question}` at runtime.

### Step 3: Turn model + prompt into a chain

You can connect components into a **chain**:

```python
from langchain_core.output_parsers import StrOutputParser

parser = StrOutputParser()

qa_chain = prompt | llm | parser  # pipeline
```

This says:

- fill the prompt template,
- call the model,
- parse the output into a string.

Now you can call:

```python
answer = qa_chain.invoke({"question": "Explain REST APIs simply."})
print(answer)
```

This wraps your logic into **one reusable unit**.

### Step 4: Add tools (your own functions)

You can wrap backend functions as tools:

```python
from langchain_core.tools import tool


@tool
def get_order_total(order_id: int) -> float:
    """Return total price for a given order."""
    # Your real business logic here
    return 42.0


tools = [get_order_total]
```

Now LangChain knows:

- the function name,
- its arguments,
- and a short description for the agent.

### Step 5: Add an agent that can call tools

Instead of you deciding when to call `get_order_total`,  
you let an **agent** decide based on user text:

```python
from langchain.agents import create_tool_calling_agent, AgentExecutor

agent_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are an order support assistant. "
               "Use tools when you need real data."),
    ("user", "{input}")
])

agent_llm = ChatOpenAI(model="gpt-4.1-mini")

agent = create_tool_calling_agent(agent_llm, tools, agent_prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

result = agent_executor.invoke({"input": "What is the total for order 123?"})
print(result["output"])
```

Here:

- LangChain:
  - passes the tool list and prompt to the LLM,
  - helps the LLM format tool calls,
  - routes tool results back into the conversation.
- You:
  - define the tools,
  - define the system message and rules,
  - integrate the agent into your API.

This is a **small agentic workflow** powered by LangChain.

---

## 6. Short code examples (minimal chain)

Here’s a very small, self‑contained example:  
“Explain something to a backend developer in simple words.”

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser


# 1) Model
llm = ChatOpenAI(model="gpt-4.1-mini")


# 2) Prompt template
prompt = ChatPromptTemplate.from_messages([
    ("system", "You explain things to backend developers using simple language."),
    ("user", "{topic}")
])


# 3) Output parser (convert to string)
parser = StrOutputParser()


# 4) Chain = prompt → model → parse
explain_chain = prompt | llm | parser


# 5) Use it in your code or API
result = explain_chain.invoke({"topic": "What is a message queue?"})
print(result)
```

This is already:

- testable,
- reusable,
- and easy to plug into FastAPI or any other service.

LangChain just makes this composition **explicit and clean**.

---

## 7. How Codex helps with LangChain

You don’t have to memorize every LangChain import or pattern.  
Codex can write most of this for you.

Examples of how Codex can help:

- **Project setup**
  - “Create a basic LangChain project structure with `app/`, `agents/`, `tools/`, `chains/`, and `tests/` folders.”

- **Chain creation**
  - “Create a chain that:
     - takes a user question,
     - calls the model,
     - and returns a JSON with fields `summary` and `action_items`.”

- **Tool wrapping**
  - “Wrap the functions in `services/reporting.py` as LangChain tools with clear docstrings.”

- **Agent creation**
  - “Build an agent that can answer support questions using:
     - tools from `tools/user_tools.py`,
     - and a prompt that discourages guessing.”

- **Refactoring**
  - “Split this large `agent.py` file into:
     - `agents/support_agent.py`,
     - `tools/support_tools.py`,
     - and update imports everywhere.”

- **Testing**
  - “Generate pytest tests for `chains/explain_chain.py` that:
     - mock the LLM call,
     - and assert we build the correct prompt.”

So LangChain provides the **framework**,  
and Codex provides the **coding muscle** to use that framework quickly.

---

## 8. Diagrams

### Basic LangChain chain

```text
[User Request]
      ↓
[Prompt Template]
      ↓
[LLM (OpenAI model)]
      ↓
[Output Parser]
      ↓
[Final Answer]
```

### LangChain with tools and agents

```text
           [User Request]
                  ↓
              [Agent]
                  ↓
        Decides which tool to use
                  ↓
   ┌───────────────────────────────────┐
   │  [Tool: get_order_total]         │
   │  [Tool: get_user_profile]        │
   │  [Tool: send_email]              │
   └───────────────────────────────────┘
                  ↓
          [Combined Result]
                  ↓
             [Final Answer]
```

### Where LangChain sits in your backend

```text
[Client] → [API (FastAPI, etc.)] → [LangChain Layer]
                                      ↓
                           [LLMs, Tools, Chains, Agents]
                                      ↓
                         [Databases / Services / APIs]
```

You still build and own the **API**, **DB**, and **services**.  
LangChain is the **orchestration layer** that organizes AI calls and tool usage.

---

## 9. Summary

- **LangChain** is a **Python framework** that helps you organize AI calls into:
  - prompts,
  - chains,
  - tools,
  - agents,
  - and memory.
- For backend developers, it turns “random AI scripts” into:
  - clean, testable components,
  - reusable pipelines,
  - and well‑structured agent workflows.
- You can think of:
  - **chains** as function pipelines or stored procedure sequences,
  - **tools** as service functions or microservices,
  - **agents** as smart controllers that decide which tools to call.
- **Codex** works together with LangChain by:
  - generating chains, agents, and tools,
  - refactoring structure,
  - and writing tests and boilerplate for you.
- If you already understand controllers, services, and workflows,  
  you are very close to understanding LangChain — it just applies those same ideas to AI‑powered logic.


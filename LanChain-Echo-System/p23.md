# 23. Orchestrating Tools + Chains + Agents

## 1. Introduction

In earlier chapters you met tools, chains, and agents as separate ideas.  
Now we connect them into one working system: a workflow where an AI agent can think, choose tools, run chains of steps, and return a clean answer to your user or API.

This “orchestration” layer is what turns a single LLM call into a real backend feature that you can log, test, and evolve.

---

## 2. Why backend developers should care

- You can let users ask for outcomes (“Check my order and refund if needed”) instead of wiring every tiny endpoint by hand.
- You re‑use existing business logic (DB queries, REST clients, services) by wrapping them as tools and plugging them into a graph.
- You keep control: the agent can only call the tools you expose, with the contracts you define.
- You can model complex flows (plan → call tools → summarize) instead of one giant, messy function.
- You get a structure that fits how you already think: services, pipelines, state, logs, and tests.

---

## 3. Real-life analogy

Picture a well‑run support center:

- **Tools** are departments: Billing, Shipping, Tech Support, CRM.
- **Chains** are standard procedures: “Issue refund”, “Update address”, “Reset password”.
- **Agent** is a senior rep who:
  - Reads the customer’s email.
  - Decides which department and procedure to use.
  - Follows the steps.
  - Writes back a clear explanation.

**Orchestration** is the operations playbook:
- If it’s about delivery → talk to Shipping, then email the user.
- If it needs more info → ask the customer, then continue.
- If something fails → log it and escalate.

LangGraph is like drawing this playbook as boxes (steps) and arrows (who runs next), then executing it in code.

---

## 4. Key concepts

- **Tool**  
  A function the agent can call, usually wrapping real backend logic:
  - DB access (`get_user_by_id`)
  - External APIs (`fetch_exchange_rate`)
  - Internal services (`calculate_discount`, `create_ticket`)

- **Chain**  
  A fixed mini‑pipeline. Input goes through a sequence of steps:
  - Clean/normalize text.
  - Call an LLM with a prompt.
  - Parse/validate the output.
  - Maybe call another tool.

- **Agent**  
  A smart controller around the LLM that:
  - Reads the user’s request.
  - Decides *which* tools and chains to use.
  - Calls them in the needed order.
  - Produces a final result.

- **Orchestration layer**  
  The glue logic that connects tools, chains, and agents:
  - What is the entry point?
  - When do we call a tool?
  - What happens on error?
  - When do we stop?

- **LangGraph**  
  A library to define this orchestration as a graph:
  - **Nodes** = steps (agent, chain, tool wrapper, custom Python function).
  - **Edges** = transitions (“run this next”).
  - Supports branching and loops (e.g. “while more questions → go back to agent”).

- **State**  
  A shared context object that moves through the graph:
  - User input, intermediate tool results, model messages, final answer.

---

## 5. Steps explained simply

1. **List the real tasks you have today**  
   Examples:
   - “Look up order by ID”
   - “Compute refund amount”
   - “Send confirmation email”

2. **Wrap tasks as tools**  
   - For each backend capability, write a small Python function.
   - Mark it as a LangChain tool so an agent can call it.

3. **Build small chains for common flows**  
   - Example: “Analyze request → extract order id → call `get_order` → format response”.
   - This is just organizing code you would normally put into one controller function.

4. **Give an agent access to tools and chains**  
   - Configure the agent with:
     - The LLM model.
     - The set of tools it’s allowed to use.
     - Instructions (“You are a helpful support assistant; use tools when needed.”).

5. **Draw the orchestration as a LangGraph**  
   - Use a graph with nodes and edges:
     - Start node receives the user’s request and builds the initial state.
     - Agent node thinks and calls tools/chains.
     - Optional extra nodes: validation, logging, business rules.
     - Final node summarizes and formats the response.

6. **Connect your HTTP API to the graph**  
   - In FastAPI/Flask/Django:
     - Parse the request.
     - Call the compiled LangGraph app with the state (e.g. `{"input": user_message}`).
     - Return `state["answer"]` or similar.

7. **Add monitoring and tests**  
   - Log:
     - Which nodes ran.
     - Which tools were called.
     - Inputs/outputs (with any needed redaction).
   - Test:
     - Tool functions as normal unit tests.
     - Chains under controlled inputs.
     - Whole graph with “fake” examples (e.g. stub tools or models).

---

## 6. Short code examples

Example: a tiny LangGraph app that:

- Exposes a tool to fetch an order status.
- Uses an agent node that can call this tool.
- Summarizes the result into a user‑friendly answer.

```python
from typing import TypedDict, List
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END


# 1) Define a backend tool
@tool
def get_order_status(order_id: str) -> str:
    """Look up the status of an order by ID."""
    # Replace with real DB or HTTP call
    if order_id == "123":
        return "shipped"
    return "processing"


# 2) Shared state moving through the graph
class OrderState(TypedDict):
    input: str          # original user question
    messages: List[str] # trace of model messages
    order_status: str   # result from the tool
    answer: str         # final human‑friendly answer


llm = ChatOpenAI(model="gpt-4.1-mini")


# 3) Agent node: decide and call tools
def agent_node(state: OrderState) -> OrderState:
    runnable = llm.bind_tools([get_order_status])

    prompt = (
        "You are an order support assistant.\n"
        "User question: {question}\n"
        "If they mention an order id, call get_order_status.\n"
    )

    result = runnable.invoke(prompt.format(question=state["input"]))

    # Read tool result if the model called the tool
    tool_calls = getattr(result, "tool_calls", [])
    status = state.get("order_status", "")

    for call in tool_calls:
        if call["name"] == "get_order_status":
            status = call["output"]

    return {
        **state,
        "messages": state.get("messages", []) + [str(result.content)],
        "order_status": status,
    }


# 4) Summarize node: produce nice final text
def summarize_node(state: OrderState) -> OrderState:
    status = state.get("order_status") or "unknown"
    summary_prompt = (
        f"User asked: {state['input']}\n"
        f"Order status: {status}\n"
        "Write a short, friendly answer."
    )
    final = llm.invoke(summary_prompt)

    return {
        **state,
        "messages": state["messages"] + [str(final.content)],
        "answer": str(final.content),
    }


# 5) Build the graph
workflow = StateGraph(OrderState)
workflow.add_node("agent", agent_node)
workflow.add_node("summarize", summarize_node)

workflow.set_entry_point("agent")
workflow.add_edge("agent", "summarize")
workflow.add_edge("summarize", END)

app = workflow.compile()


# 6) Use from your web framework
def handle_http_request(user_message: str) -> str:
    state = app.invoke({"input": user_message})
    return state["answer"]
```

You can imagine replacing `get_order_status` with any backend function: queries, aggregations, or calls to other microservices.

---

## 7. How Codex helps

When you orchestrate tools, chains, and agents, Codex can:

- Suggest tool interfaces that match your existing modules and services.
- Generate LangChain tool wrappers around database functions or HTTP clients.
- Scaffold the LangGraph structure (nodes, edges, state types) from a plain‑English description of your flow.
- Refactor large controller functions into:
  - Smaller tool functions.
  - Reusable chains for common sub‑flows.
  - A clean graph that coordinates everything.
- Propose better state models (e.g. `TypedDict` or Pydantic) to keep your data well‑structured.
- Generate tests that:
  - Call tools directly.
  - Run chains with mocked tools.
  - Run the whole graph and assert on the final answer shape.

This lets you stay focused on business rules while Codex does the heavy lifting of wiring and refactoring.

---

## 8. Small diagram

High‑level architecture:

```text
User → HTTP API → LangGraph App
                 ↓
            [Agent Node]
                 ↓
        calls tools & chains
          ↙            ↘
[Tool: get_order]   [Other tools]
                 ↓
          [Summarize Node]
                 ↓
            API Response
```

Logical flow:

```text
[Start]
  ↓
[Agent decides]
  ↓
[Call tools / chains]
  ↓
[Summarize + format]
  ↓
[Finish]
```

---

## 9. Summary

- Tools, chains, and agents map well to services, procedures, and smart controllers you already know from backend work.
- Orchestration is the glue that connects them into a predictable flow: from user request to final answer.
- LangGraph lets you describe this flow as a graph of nodes and edges with shared state.
- Your HTTP API just calls the graph, and the graph coordinates LLM reasoning plus tool usage.
- Codex can help you design, generate, and refactor this orchestration so you can move faster while keeping the codebase understandable for your whole team.


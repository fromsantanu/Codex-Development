# 12. Creating Memory in Agents

## 1. Simple Intro

Out of the box, most LLM calls are **stateless**:

- they only see the current request you send,
- they “forget” previous messages unless you resend them.

**Memory** is how we give an agent a **notebook** so it can:

- remember the conversation so far,
- remember important state (user, context, intermediate results),
- behave more like a real assistant instead of answering each request in isolation.

In this chapter we’ll look at:

- **conversation memory** (chat history),
- **state memory** (extra data your backend keeps),
- when to use each,
- and how Codex and LangChain help.

---

## 2. Why Backend Devs Should Learn This

As a backend developer you already think about:

- sessions,
- authentication and user profiles,
- shopping carts,
- long‑running workflows (orders, tickets, jobs).

Agents need the same ideas:

- **Conversation memory** → like keeping the entire support ticket thread.
- **State memory** → like storing user preferences, IDs, or partial results.

With good memory you can:

- avoid repeating questions (“What’s your name?” every message),
- make responses more personal (“Hi Alice, welcome back.”),
- run multi‑step flows (“First ask questions, then generate a report.”),
- hide complexity in your backend while the agent appears “smart”.

Codex can help you plug memory into your existing APIs **without rewriting everything**.

---

## 3. Analogy: Barista with a Notebook

Imagine a coffee shop:

- A new barista talks to you, but forgets you after each sentence.  
  Every time you say something, they ask “What’s your order again?”
- A good barista remembers:
  - your name,
  - your usual drink,
  - that you don’t like sugar.

Now add a **notebook**:

- If the barista forgets, they check the notebook.
- They also write down new info as you talk.

For agents:

- **Conversation memory** = the transcript of your chat (what you and the agent said).
- **State memory** = extra notes about you (user_id, preferences, last result, etc.).

Together they make the agent feel like the same “person” across requests.

---

## 4. Key Ideas About Memory

1. **Stateless vs stateful**  
   - Stateless: each request is independent (pure function).  
   - Stateful: behavior depends on what happened before (session, history).

2. **Conversation memory**  
   - Stores the **chat messages** between user and agent.  
   - Used to keep context: pronouns, previous questions, decisions.  
   - Often limited to the **recent window** to save tokens.

3. **State memory**  
   - Stores **structured data** outside the conversation:  
     user IDs, flags, configurations, computed results.  
   - Lives in your own storage: dict, cache, DB, Redis, etc.

4. **Short‑term vs long‑term**  
   - Short‑term: last N messages (like RAM).  
   - Long‑term: durable storage (DB, vector store) you can reload later.

5. **Cost and safety**  
   - More memory → more tokens → higher cost and latency.  
   - You must treat memory like any user data: **privacy, retention, PII**.

6. **Where LangChain fits**  
   - Gives you components for **chat history** and **stateful chains/agents**.  
   - Handles wiring history into prompts so you don’t build it by hand every time.

7. **Where Codex fits**  
   - Converts your current stateless handlers into stateful ones.  
   - Adds memory classes, storage adapters, and tests with minimal boilerplate.

---

## 5. Step‑by‑Step: Adding Memory to an Agent

We’ll focus on a simple chat‑style agent.

### Step 1: Decide What to Remember

Ask:

- Do I only need **recent conversation**?  
  (e.g. follow‑up questions, pronouns)
- Do I need **user profile or computed data**?  
  (e.g. “user_123 has plan = PRO”)

Write this down as a small list:

- “Remember last 10 messages.”
- “Remember user name and plan.”

### Step 2: Choose a Storage Strategy

For **conversation memory**:

- For quick demos: in‑memory store (Python dict in the process).
- For production: Redis, DB, or other shared store per session/user.

For **state memory**:

- Use what you already know:  
  SQL row, Redis hash, JSON document in a DB, etc.

### Step 3: Add Conversation Memory to a Chain

In LangChain you typically:

- build a normal chain (prompt + model),
- wrap it with a **message history helper** that:
  - records each message,
  - replays recent messages into the next prompt.

This gives you a **chatbot with memory**.

### Step 4: Add State Memory from Your Backend

You might:

- load user data from DB using `user_id`,
- merge it into the prompt as extra context,
- update that data when the user changes something.

State memory is often **your code**, not LangChain’s:

- LangChain handles conversation history.
- Your backend handles persistent business state.

### Step 5: Decide When to Reset Memory

You should decide:

- When to clear conversation memory (e.g. “New topic” button).
- How much history to keep (e.g. last 10 or 20 messages).
- Which fields in state memory you **never** overwrite (like audit logs).

Codex can help implement reset logic and cap sizes safely.

### Step 6: Plug Into Your API or Agent

- Wrap your chain with memory.  
- Call it from FastAPI, or register it as a tool.  
- Or use it inside a bigger agent that calls other tools and chains.

---

## 6. Short Code Examples

Below examples use the **modern LangChain “runnable” style**.

### Example 1: Conversation Memory with `RunnableWithMessageHistory`

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables import RunnableWithMessageHistory

llm = ChatOpenAI(model="gpt-4.1-mini")

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a friendly support agent."),
    ("placeholder", "{history}"),  # previous messages
    ("user", "{input}")            # new user message
])

chain = prompt | llm | StrOutputParser()

store = {}  # simple in-memory store; replace with Redis/DB in real apps

def get_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]

chat_with_memory = RunnableWithMessageHistory(
    chain,
    get_history,
    input_messages_key="input",
    history_messages_key="history",
)

config = {"configurable": {"session_id": "user-123"}}

print(chat_with_memory.invoke({"input": "Hi, my name is Alice."}, config=config))
print(chat_with_memory.invoke({"input": "What is my name?"}, config=config))
```

Behavior:

- First call: agent learns “Alice”.
- Second call: the history is replayed; the agent can answer “Your name is Alice.”

You can now call this from a FastAPI route using a cookie, token, or session ID.

---

### Example 2: Simple State Memory from Your Backend

Here we keep a tiny **user profile dict** and inject it into the prompt every time.

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

llm = ChatOpenAI(model="gpt-4.1-mini")
parser = StrOutputParser()

user_state = {
    # In real apps this would be a DB or cache
    "user-123": {"name": "Alice", "plan": "PRO"},
}

prompt = ChatPromptTemplate.from_messages([
    ("system",
     "You are a helpful backend assistant. "
     "You know the user's profile and personalize responses."),
    ("user",
     "User profile:\n"
     "Name: {name}\n"
     "Plan: {plan}\n\n"
     "User question: {question}")
])

chain = prompt | llm | parser

def answer_with_state(user_id: str, question: str) -> str:
    profile = user_state.get(user_id, {"name": "Guest", "plan": "FREE"})
    return chain.invoke({
        "name": profile["name"],
        "plan": profile["plan"],
        "question": question,
    })

print(answer_with_state("user-123", "What features do I have access to?"))
```

Here:

- Conversation memory is not used.
- But **state memory** (name + plan) shapes the answer.

Later you can:

- update `user_state` when plan changes,
- load from a real DB instead of a dict,
- and combine this with conversation memory if needed.

---

## 7. How Codex Helps

Codex is very handy when adding memory to existing projects:

- **Refactoring stateless handlers**  
  - “Take this FastAPI endpoint and add conversation memory using `RunnableWithMessageHistory`.”

- **Wiring up storage backends**  
  - “Replace this in-memory dict with Redis for chat history; add a small helper class.”

- **Designing what to remember**  
  - “Suggest which fields of this `User` model should be passed into the agent as state memory.”

- **Capping and clearing history**  
  - “Add logic to keep only the last 10 messages per session and clear memory on logout.”

- **Testing memory behavior**  
  - “Create pytest tests to ensure the agent remembers the user’s name across 3 messages.”

You describe **what you want to remember**; Codex writes and refines the code so memory fits naturally into your existing backend structure.

---

## 8. Small Diagram

### Conversation Memory in the Flow

```text
User → API → Agent Chain
             ↑     ↓
        Read/Write Memory
             ↑
      (Chat History Store)
```

### Combining Conversation + State Memory

```text
           ┌──────────────┐
User → API │  Agent Chain │
           └──────┬───────┘
                  │
        ┌─────────┴─────────┐
        │                   │
 [Conversation History]  [State Memory]
   (messages)            (user profile,
                          last results,
                          flags, etc.)
```

---

## 9. Summary

- LLMs are stateless by default; **memory** makes agents feel consistent and personal across messages.
- **Conversation memory** stores the chat history; **state memory** stores structured data like user profile and workflow state.
- You choose what to remember, how long to keep it, and where to store it (in‑memory, Redis, DB).
- LangChain provides helpers like `RunnableWithMessageHistory` and chat history classes so you don’t have to wire memory manually.
- Codex can refactor your existing code to add memory, connect it to storage, and generate tests so your agentic workflows stay robust over time.


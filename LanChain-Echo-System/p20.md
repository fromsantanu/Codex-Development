# 20. Event-driven Agents with LangGraph

## 1. Introduction

Most of the time we think of an agent as:

- you send a **request**,
- it thinks, calls tools, returns a **single answer**,
- and the conversation is done.

But many real systems are **event-driven**:

- new data arrives from a webhook,
- a payment succeeds or fails,
- a user replies to an email later,
- a long job finishes and sends a callback.

Event-driven agents with **LangGraph** let you:

- model these events as a **graph of steps**,
- keep **state** between events,
- resume the workflow when something new happens,
- and let agents **react** to events over time instead of in one shot.

---

## 2. Why Backend Developers Should Care

If you’ve built backends, you already use event-driven ideas:

- webhooks from Stripe or GitHub,
- messages in RabbitMQ / Kafka,
- cron jobs that trigger tasks,
- “status changed” events in your domain.

Event-driven agents with LangGraph give you:

- a way to **attach LLM logic** to these events,
- a visualizable **graph** instead of tangled `if/else`,
- built-in **state management** (checkpoints),
- a clean path to **pause / resume / retry** flows.

Instead of hard-coding everything into one huge handler, you:

- break logic into **nodes** (small Python functions or agents),
- connect them with **edges**,
- and let LangGraph route the flow whenever an event arrives.

---

## 3. Real-life Analogy

Imagine a **smart package delivery system**:

Events:

- `PACKAGE_CREATED`
- `PACKAGE_PICKED_UP`
- `PACKAGE_IN_TRANSIT`
- `PACKAGE_DELIVERED`
- `DELIVERY_FAILED`

For each event:

- some **small routine** runs,
- it updates the **package record**,
- maybe sends an email or SMS,
- maybe asks a **human operator** for help.

The package has a **tracking file** (its state):

- who is the recipient,
- current location,
- previous events,
- any notes.

LangGraph is like the **workflow map** for this system:

- each box is a step (node),
- arrows show where to go next (edges),
- the tracking file is the **shared state**,
- new events “wake up” the workflow and move it forward.

Event-driven agents are just this idea, but with LLMs making some of the decisions.

---

## 4. Key Concepts

Here are the main ideas in event-driven agents with LangGraph, translated into backend terms.

- **State**
  - A structured object (often a `TypedDict`) that stores:
    - user info,
    - current step,
    - previous events,
    - partial results.
  - Similar to a **workflow context** or **job record**.

- **Node**
  - A Python function (or agent) that:
    - receives the current state,
    - does some work (LLM call, tool call, API call),
    - returns an updated state.
  - Like a **handler** in a pipeline.

- **Edge**
  - A connection between nodes.
  - Tells LangGraph **which node runs next**.
  - Can be:
    - fixed (always go A → B),
    - conditional (go to different nodes based on state).

- **Graph**
  - Collection of nodes, edges, and rules.
  - Similar to a **workflow diagram** in code.

- **Event**
  - Something that happens **over time**:
    - user sends a new message,
    - payment is confirmed,
    - a background job finishes.
  - You handle it by calling the graph again with:
    - the same **thread / conversation id**,
    - a new **input payload**,
    - LangGraph resumes from the stored state.

- **Checkpoint / Persistence**
  - LangGraph can store state between runs:
    - in memory for demos,
    - in a DB for production.
  - Lets you:
    - pause and resume,
    - recover after crashes,
    - handle long-lived workflows.

---

## 5. Steps Explained Simply

Here is a step-by-step way to build an event-driven agent with LangGraph.

### Step 1: Define the Use Case

Pick a simple, event-driven scenario, for example:

- “When a user submits a support ticket:
  - acknowledge it,
  - analyze urgency,
  - wait for a human agent decision,
  - then send a final response.”

Events might be:

- `TICKET_CREATED`
- `HUMAN_DECISION`

### Step 2: Design the State

Create a clear state structure, e.g.:

```python
state = {
    "ticket_id": "",
    "user_message": "",
    "ticket_summary": "",
    "urgency": "",
    "human_decision": "",
    "final_reply": "",
}
```

This is the “single source of truth” your nodes will read and update.

### Step 3: Write Node Functions

For each logical step, write a small function:

- `summarize_ticket(state)` – uses an LLM to summarize.
- `classify_urgency(state)` – uses an LLM or rules.
- `apply_human_decision(state)` – merges a human’s choice into the reply.

Each function:

- accepts the current state,
- returns a new or updated state.

### Step 4: Build the Graph with LangGraph

Use `StateGraph` to:

- declare your state type,
- register nodes (functions),
- connect edges (who runs after who),
- set the entry node for the flow.

Now you have a **reusable workflow** in code.

### Step 5: Add a Checkpointer

To make it truly event-driven:

- plug in a checkpointer (e.g. in-memory or DB-backed),
- include a **thread id** or **ticket id** in `config`,
- so the same workflow instance can be resumed later.

When a new event arrives (e.g. `HUMAN_DECISION`):

- you call the graph again with the same id,
- LangGraph loads the previous state,
- runs the next nodes,
- and updates the state.

### Step 6: Connect It to Your Backend

Finally, wire the graph into your app:

- HTTP endpoint for `TICKET_CREATED` → call the graph with initial state.
- HTTP endpoint for `HUMAN_DECISION` → call the graph again with added fields.

Your backend remains in control:

- you decide when to invoke the graph,
- you pass in external events,
- the graph manages the **agentic workflow** between events.

---

## 6. Short Code Examples

These examples show a very small event-driven support workflow using LangGraph.

### Example 1: Define State and Nodes

```python
# graph/support_state.py
from typing import TypedDict


class SupportState(TypedDict, total=False):
    ticket_id: str
    user_message: str
    ticket_summary: str
    urgency: str
    human_decision: str
    final_reply: str
```

```python
# graph/support_nodes.py
from langchain_openai import ChatOpenAI

from .support_state import SupportState


llm = ChatOpenAI(model="gpt-4.1-mini")


def summarize_ticket(state: SupportState) -> SupportState:
    """LLM summarizes the user's message."""
    prompt = (
        "Summarize this support ticket in 2-3 sentences:\n\n"
        f"{state.get('user_message', '')}"
    )
    summary = llm.invoke(prompt).content
    state["ticket_summary"] = summary
    return state


def classify_urgency(state: SupportState) -> SupportState:
    """LLM labels the ticket as low/medium/high."""
    prompt = (
        "Given this ticket summary, classify urgency as "
        "'low', 'medium', or 'high'. Reply with one word.\n\n"
        f"{state.get('ticket_summary', '')}"
    )
    urgency = llm.invoke(prompt).content.strip().lower()
    state["urgency"] = urgency
    return state


def build_final_reply(state: SupportState) -> SupportState:
    """Combine model understanding + human decision into a reply."""
    decision = state.get("human_decision", "pending review")
    prompt = (
        "You are a support agent.\n"
        "Write a friendly reply using this info.\n\n"
        f"Summary: {state.get('ticket_summary', '')}\n"
        f"Urgency: {state.get('urgency', '')}\n"
        f"Human decision: {decision}\n"
    )
    reply = llm.invoke(prompt).content
    state["final_reply"] = reply
    return state
```

### Example 2: Build the LangGraph Workflow

```python
# graph/support_graph.py
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

from .support_state import SupportState
from .support_nodes import (
    summarize_ticket,
    classify_urgency,
    build_final_reply,
)


def build_support_graph():
    builder = StateGraph(SupportState)

    builder.add_node("summarize_ticket", summarize_ticket)
    builder.add_node("classify_urgency", classify_urgency)
    builder.add_node("build_final_reply", build_final_reply)

    # Simple linear flow for now
    builder.set_entry_point("summarize_ticket")
    builder.add_edge("summarize_ticket", "classify_urgency")
    builder.add_edge("classify_urgency", "build_final_reply")
    builder.add_edge("build_final_reply", END)

    memory = MemorySaver()  # swap with DB-backed in production
    return builder.compile(checkpointer=memory)
```

### Example 3: Use the Graph in an Event-driven Way

```python
# app/support_events.py
from graph.support_graph import build_support_graph


graph = build_support_graph()


def on_ticket_created(ticket_id: str, user_message: str) -> dict:
    # Start a new workflow instance for this ticket
    state = {
        "ticket_id": ticket_id,
        "user_message": user_message,
    }
    config = {"configurable": {"thread_id": ticket_id}}
    return graph.invoke(state, config=config)


def on_human_decision(ticket_id: str, decision: str) -> dict:
    # Resume the same workflow with a new event
    state_update = {"human_decision": decision}
    config = {"configurable": {"thread_id": ticket_id}}
    return graph.invoke(state_update, config=config)
```

What’s happening:

- `on_ticket_created` starts the graph:
  - summarizes the ticket,
  - classifies urgency,
  - and builds a first version of the reply.
- LangGraph stores the state under this `thread_id`.
- Later, `on_human_decision`:
  - loads the existing state,
  - adds the human’s decision,
  - runs `build_final_reply` again,
  - and produces an updated message.

Your backend just calls `on_ticket_created` and `on_human_decision` from your HTTP handlers or queue consumers.

---

## 7. How Codex Helps

Codex is very good at turning messy, event-driven logic into clean LangGraph workflows.

You can ask Codex to:

- **Extract a workflow from existing code**
  - “Look at this 200-line webhook handler and propose a LangGraph with nodes for each major step.”

- **Generate state and graph definitions**
  - “Create a `TypedDict` for support ticket state and a `StateGraph` that runs summarize → classify → reply.”

- **Add persistence**
  - “Swap the in-memory checkpointer with a simple SQLite or Postgres-backed checkpointer, and show me how to configure `thread_id`.”

- **Refactor to event handlers**
  - “Create helper functions `on_ticket_created` and `on_human_decision` that invoke the graph with the correct config.”

- **Add tests**
  - “Write pytest tests that simulate events and assert that the graph updates `urgency` and `final_reply` correctly.”

Codex can read your current handlers, build the initial graph, and then keep refactoring as your workflow grows.

---

## 8. Small Diagram

### Event-driven Support Workflow with LangGraph

```text
      [HTTP: TICKET_CREATED]
                 ↓
       on_ticket_created()
                 ↓
        ┌───────────────────┐
        │  LangGraph (G)    │
        │  State: Support   │
        └───────────────────┘
           ↓      ↓      ↓
     [summarize]→[classify]→[build_reply]
                 (nodes)
                 ↓
           state saved (checkpoint)
```

### Later Event

```text
[HTTP: HUMAN_DECISION]
          ↓
 on_human_decision()
          ↓
    LangGraph loads state
          ↓
  [build_reply] node runs
          ↓
     Updated final_reply
```

---

## 9. Summary

- Many real systems are **event-driven**, not just one-off requests.
- LangGraph lets you model event-driven agents as a **graph** of nodes and edges with **state** passed between them.
- Each event (webhook, user action, job result) can **resume** the same workflow instance using checkpoints and a thread id.
- As a backend developer, this feels similar to **workflow engines** and **message-driven architectures**, but with LLMs and tools inside the nodes.
- Codex helps design the state, nodes, edges, and event handlers, and can refactor existing handlers into a clean, testable LangGraph-based, event-driven agent system.


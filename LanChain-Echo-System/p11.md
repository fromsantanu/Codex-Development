# 11. Creating Chains

## 1. Simple Intro

So far you’ve seen:

- prompts (what to ask),
- tools (what actions the agent can take).

Now we add **chains**: a **series of steps** you connect together, where
the **output of one step becomes the input of the next**.

In LangChain, a chain might:

- take user input,
- format a prompt,
- call an LLM (like Codex),
- post‑process the answer,
- maybe call a tool,
- then return a clean result.

Think of a chain as a **small workflow function** that you can call from your backend or from an agent.

---

## 2. Why Backend Devs Should Learn This

As a backend developer you already build:

- request → service → repository flows,
- middleware pipelines,
- stored procedures calling other queries.

Chains give you similar structure for **LLM logic**:

- Make LLM behavior **predictable and testable**.
- Reuse the same workflow from API routes, workers, or agents.
- Keep “AI messiness” wrapped in one clean unit.
- Plug chains into **agents** so the agent can call a whole flow as one step.

Codex can then help you:

- generate the boilerplate chains,
- refactor long functions into clean steps,
- add tests around each step.

---

## 3. Analogy: Factory Assembly Line

Imagine a simple factory that builds a product:

1. **Step 1 – Cut material**
2. **Step 2 – Assemble parts**
3. **Step 3 – Paint**
4. **Step 4 – Quality check**

The item moves **station by station** until it’s done.

Chains work the same way:

- Input comes in at the start.
- Each step (prompt, LLM call, tool call) does **one clear job**.
- At the end you get a clean result you can return from your API.

Some lines are:

- **sequential** (one station after another),
- **parallel** (multiple stations work at the same time),
- **simple single‑step** (just one LLM call = LLMChain).

---

## 4. Key Ideas About Chains

1. **Chain = series of steps**  
   A chain is just **connected callables** (prompt, LLM, tool, Python function).

2. **LLMChain = prompt + model**  
   The simplest chain: takes input variables → fills a prompt → calls an LLM.

3. **Sequential chain**  
   Steps run **one after another**: Step A → Step B → Step C.  
   Example: *”Clean user question → decide SQL query → explain result.”*

4. **Parallel chain**  
   Run **independent branches at the same time** and then combine results.  
   Example: *”Given a support ticket, create a summary AND detect sentiment AND assign priority.”*

5. **Inputs and outputs are explicit**  
   Chains define what they expect and what they return.  
   That makes them easier to **test** and to **plug into APIs**.

6. **Chains vs agents**  
   - Chain: fixed recipe, same steps every time.  
   - Agent: flexible, decides which tools/steps to use.  
   You often build **chains** first, then let an **agent** call them.

7. **Composable with `|`**  
   In modern LangChain, you often build chains by piping components together using `|`, like Unix pipes or middleware.

---

## 5. Step‑by‑Step: Building Chains

We’ll go from **single LLMChain** → **sequential chain** → **parallel chain**.

### Step 1: Start with a Single LLMChain

Use one model call to do one job, e.g. “rewrite text in a friendlier tone”.

- Input: raw text.
- Output: rewritten text.

This is already a **chain**: prompt → LLM → plain string.

### Step 2: Add a Second Step (Sequential)

Maybe you want:

1. Step A: Turn a messy user question into a **structured task description**.
2. Step B: Use that structured description to **generate code** or an answer.

You can connect two chains:

- First chain: “normalize the request”.
- Second chain: “produce the final output”.

### Step 3: Add a Tool Step

Now you want to:

1. Clean the user question.
2. Generate a SQL query from it.
3. Run the SQL against a DB (tool).
4. Ask an LLM to explain the result in plain language.

Here your chain mixes:

- LLM calls,
- your own Python code,
- tools (e.g. database reader).

### Step 4: Introduce Parallel Steps

Sometimes you have **one input** but want **multiple independent views**:

- Summary of the text.
- Sentiment (positive/negative).
- List of tags.

You can create a **parallel chain** that:

- sends the same text to 2–3 small sub‑chains,
- collects all results into one dict,
- returns that to your API or agent.

### Step 5: Wrap as a Service

Once your chain is stable, treat it like a **service function**:

- call it from FastAPI or your worker,
- or register it as a tool that an agent can use,
- write tests against the chain’s inputs/outputs.

Codex is great at turning your **existing spaghetti logic** into a clear chain with named steps.

---

## 6. Short Code Examples

Below are small, practical snippets.  
Assume you already installed `langchain`, `langchain-openai`, and set `OPENAI_API_KEY`.

### Example 1: Simple LLMChain (Prompt + Model)

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

llm = ChatOpenAI(model="gpt-4.1-mini")  # or a Codex-like model

prompt = ChatPromptTemplate.from_messages([
    ("system", "You rewrite text in a friendly, simple tone."),
    ("user", "Please rewrite this: {text}")
])

chain = prompt | llm | StrOutputParser()

result = chain.invoke({"text": "THE SERVICE IS TEMPORARILY UNAVAILABLE DUE TO MAINTENANCE"})
print(result)
```

Here:

- `prompt` defines the template.
- `llm` is the model (could be a Codex model).
- `StrOutputParser` turns the chat result into a plain string.

This is an **LLMChain** in the modern runnable style.

---

### Example 2: Sequential Chain (Outline → Draft)

We’ll build two small chains and then call them one after another.

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

llm = ChatOpenAI(model="gpt-4.1-mini")
parser = StrOutputParser()

outline_prompt = ChatPromptTemplate.from_messages([
    ("system", "You create short outlines for technical blog posts."),
    ("user", "Create a bullet-point outline for a blog post about: {topic}")
])
outline_chain = outline_prompt | llm | parser

draft_prompt = ChatPromptTemplate.from_messages([
    ("system", "You write short, clear blog post sections."),
    ("user", "Write a short blog post using this outline:\n\n{outline}")
])
draft_chain = draft_prompt | llm | parser

def generate_blog_post(topic: str) -> str:
    outline = outline_chain.invoke({"topic": topic})
    draft = draft_chain.invoke({"outline": outline})
    return draft

print(generate_blog_post("Using agents to automate support tickets"))
```

This is a **sequential chain** implemented in plain Python:

- First chain: generate outline.
- Second chain: turn outline into a draft.

You can later wrap `generate_blog_post` in FastAPI or expose it as a tool.

---

### Example 3: Parallel Chain (Summary + Sentiment)

Use `RunnableParallel` to run multiple branches at once.

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel

llm = ChatOpenAI(model="gpt-4.1-mini")
parser = StrOutputParser()

summary_prompt = ChatPromptTemplate.from_messages([
    ("system", "You summarize support tickets in one sentence."),
    ("user", "{text}")
])
summary_chain = summary_prompt | llm | parser

sentiment_prompt = ChatPromptTemplate.from_messages([
    ("system", "You classify sentiment as positive, neutral, or negative."),
    ("user", "{text}")
])
sentiment_chain = sentiment_prompt | llm | parser

parallel_chain = RunnableParallel(
    summary=summary_chain,
    sentiment=sentiment_chain,
)

result = parallel_chain.invoke({"text": "Customer is very upset that the API was down all weekend."})
print(result)
# {'summary': '...', 'sentiment': 'negative'}
```

This is a **parallel chain**:

- Both branches receive the same `text`.
- Each does a different job.
- You get a single dict with all results.

---

## 7. How Codex Helps

Codex is extremely helpful when working with chains:

- **Designing the steps**  
  - “Given this function, break it into 3 chain steps: normalize input, call LLM, post-process.”

- **Converting ad‑hoc code into chains**  
  - “Refactor this script into a LangChain runnable pipeline using `ChatPromptTemplate` and `RunnableParallel`.”

- **Adding new branches**  
  - “Take this sequential chain and add a parallel branch that tags the text with 3 keywords.”

- **Writing tests**  
  - “Generate pytest tests for this chain given a couple of example inputs/outputs.”

- **Integrating with your backend**  
  - “Create a FastAPI endpoint that calls `generate_blog_post` and returns JSON.”

Treat Codex like a **workflow architect + code generator**:

- you describe the steps in plain language,
- Codex writes the chain code,
- you review, adjust, and add tests.

---

## 8. Small Diagram

### Sequential Chain

```text
User Input
   ↓
[Step 1: Normalize Question]
   ↓
[Step 2: Generate SQL]
   ↓
[Step 3: Run DB Tool]
   ↓
[Step 4: Explain Result]
   ↓
API Response
```

### Parallel Chain

```text
             ┌─────────────────────┐
User Input → │  Parallel Chain     │
             └───────┬─────────────┘
                     │
      ┌──────────────┴───────────────┐
      │                              │
[Summary Chain]               [Sentiment Chain]
      │                              │
      └──────────────┬───────────────┘
                     ↓
            Combined Result Dict
                     ↓
                API / Agent
```

---

## 9. Summary

- A **chain** is a series of connected steps (prompts, models, tools, Python code) that turns messy input into clean output.
- **LLMChain** is the simplest chain: just a prompt plus a model; you can build bigger workflows on top of it.
- **Sequential chains** run steps one after another; **parallel chains** run independent branches at the same time and combine results.
- Chains make LLM behavior **predictable, testable, and reusable** across your backend, agents, and workers.
- Codex can help you design, generate, refactor, and test chains so your agentic workflows stay clean and maintainable as they grow.


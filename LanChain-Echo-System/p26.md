# 26. Data Analysis Agent

## 1. Introduction

Many AI examples focus on chatting, but a lot of real backend work is about **understanding data**: tables, logs, metrics, CSV files, and query results.  
A **data analysis agent** is an assistant that can read data from your systems, run simple computations, and explain the results in plain language.

Instead of you manually writing ad‑hoc scripts or SQL each time, the agent can help you explore datasets, spot patterns, and answer “why” questions about the numbers.

---

## 2. Why backend developers should care

- You often need quick answers from data:
  - “Which endpoints are slowest this week?”
  - “How many signups failed due to payment errors?”
  - “Is this new feature improving conversion?”
- A data analysis agent can:
  - Call your existing data sources (SQL, logs, CSV exports).
  - Compute summaries and simple statistics.
  - Turn raw results into clear explanations and recommendations.
- You stay in control:
  - You define which data sources and queries are allowed.
  - You keep heavy lifting in your DB / data warehouse.
  - The agent mainly coordinates queries and interpretation.

---

## 3. Real-life analogy

Imagine you have a **data analyst teammate**:

- You ask:
  - “Can you check our error rate over the last 7 days?”
  - “Which country has the highest churn?”
- They:
  - Open dashboards and databases.
  - Run a few queries.
  - Maybe export a CSV and do quick calculations.
  - Come back with: “Here’s what I found, and here’s why it might be happening.”

A data analysis agent behaves like that analyst:
- **Tools** are its access to databases, logs, and files.
- **Reasoning** is how it decides which query to run next.
- **Answer** is a human‑readable explanation + key numbers.

---

## 4. Key concepts

- **Data analysis agent**  
  An agent specialized for reading and interpreting structured or semi‑structured data:
  - Tables (SQL results).
  - CSV/Parquet exports.
  - Logs, metrics, and events.

- **Data tools**  
  Functions that let the agent interact with your data stack, for example:
  - `run_sql(query: str) -> str`
  - `load_csv_preview(path: str) -> str`
  - `get_metric_timeseries(name: str, days: int) -> str`

- **Schema and metadata**  
  Information about:
  - Which tables exist.
  - Column names and types.
  - Units, timezones, and semantics.
  The agent uses this to form valid, meaningful queries.

- **Descriptive analysis**  
  Answering “what happened?” with:
  - counts, sums, averages, percentiles
  - top N items
  - basic trends

- **Diagnostic analysis**  
  Answering “why did it happen?”:
  - comparing segments (country, device, plan)
  - looking for correlations and patterns
  The agent helps by turning numbers into hypotheses and explanations.

- **Guardrails and privacy**  
  Rules that limit:
  - Which tables/columns can be queried.
  - Whether raw PII is exposed.
  - Maximum result sizes and query time.

---

## 5. Steps explained simply

1. **Decide which data sources to expose**  
   Examples:
   - Production or analytics database (read‑only).
   - Metrics store or monitoring API.
   - S3 bucket with CSV exports.

2. **Wrap each data access as a tool**  
   - Define small functions for:
     - Running safe, parameterized queries.
     - Loading a sample of rows.
     - Fetching aggregates or time series.
   - Tools should return *compact* text summaries, not huge raw dumps.

3. **Describe your schema to the agent**  
   - Provide:
     - Table names.
     - Important columns and their meaning.
   - This helps the agent decide *which* tool and parameters to use.

4. **Design the agent’s role and prompts**  
   - Clearly state:
     - “You are a data analysis assistant.”
     - “Use tools to fetch data; do not invent numbers.”
     - “Explain results with both numbers and plain‑language conclusions.”

5. **Use LangGraph to manage state**  
   - State holds:
     - The user’s question.
     - Any tool results collected.
     - The final explanation.
   - You can add extra nodes for:
     - Validation (e.g. check for missing parameters like date range).
     - Follow‑up questions (“Which country do you care about?”).

6. **Connect your API or internal UI**  
   - Backend:
     - Accept a data question.
     - Pass it into the graph as `{"question": ..., "tool_results": []}`.
     - Return the final answer and key metrics.

7. **Test with realistic questions**  
   - Use real or anonymized data.
   - Ask:
     - “What changed in the last 7 days?”
     - “Which segment has the highest error rate?”
   - Check:
     - Are queries valid and efficient?
     - Is the explanation accurate and easy to understand?

---

## 6. Short code examples

Below is a small example of a data analysis agent that:
- Has tools to preview a CSV and compute a simple column average.
- Lets the LLM decide when to call these tools.
- Returns a human‑readable explanation.

```python
from typing import TypedDict, List
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
import csv
import statistics


# 1) Data tools (simple CSV-based demo)
@tool
def load_csv_preview(path: str, rows: int = 5) -> str:
    """Preview the first few rows of a CSV file."""
    try:
        with open(path, newline="") as f:
            reader = csv.reader(f)
            header = next(reader, [])
            sample = [row for _, row in zip(range(rows), reader)]
    except FileNotFoundError:
        return f"File not found: {path}"
    return f"Columns: {header}\nSample rows: {sample}"


@tool
def column_mean(path: str, column_index: int) -> str:
    """Compute the mean of a numeric column (by index) in a CSV file."""
    try:
        with open(path, newline="") as f:
            reader = csv.reader(f)
            header = next(reader, [])
            values = []
            for row in reader:
                try:
                    values.append(float(row[column_index]))
                except (ValueError, IndexError):
                    continue
        if not values:
            return "No numeric values found."
        mean_val = statistics.mean(values)
    except FileNotFoundError:
        return f"File not found: {path}"
    return f"Column: {header[column_index]}, Mean: {mean_val}"


# 2) State for a single analysis request
class DataState(TypedDict):
    question: str
    messages: List[str]
    insights: List[str]  # text from tools and model
    answer: str


llm = ChatOpenAI(model="gpt-4.1-mini")


# 3) Agent node: decides which tools to use
def analysis_node(state: DataState) -> DataState:
    system_prompt = (
        "You are a data analysis assistant.\n"
        "You can inspect CSV files and compute basic statistics using tools.\n"
        "Use tools instead of inventing numbers.\n"
        "Explain your findings clearly to a backend developer.\n"
    )

    runnable = llm.bind_tools([load_csv_preview, column_mean])

    result = runnable.invoke(
        [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": state["question"]},
        ]
    )

    reply = str(result.content)

    return {
        **state,
        "messages": state.get("messages", []) + [reply],
        "insights": state.get("insights", []) + [reply],
    }


# 4) Summarize node: produce a final answer
def summarize_node(state: DataState) -> DataState:
    insights_text = "\n".join(state.get("insights", []))
    summary_prompt = (
        "You are summarizing a data investigation.\n"
        "Based on the notes below, produce a concise explanation "
        "of what the data shows and any obvious next steps.\n\n"
        f"Notes:\n{insights_text}\n"
    )
    result = llm.invoke(summary_prompt)
    answer = str(result.content)

    return {
        **state,
        "messages": state["messages"] + [answer],
        "answer": answer,
    }


# 5) Build the graph
workflow = StateGraph(DataState)
workflow.add_node("analyze", analysis_node)
workflow.add_node("summarize", summarize_node)

workflow.set_entry_point("analyze")
workflow.add_edge("analyze", "summarize")
workflow.add_edge("summarize", END)

app = workflow.compile()


# 6) Use from your backend
def analyze_question(question: str) -> str:
    state = app.invoke(
        {
            "question": question,
            "messages": [],
            "insights": [],
            "answer": "",
        }
    )
    return state["answer"]
```

You would replace the CSV tools with:
- Real SQL queries against your analytics DB.
- Calls to your metrics/logs API.
- Functions that return small, focused summaries.

---

## 7. How Codex helps

When building a data analysis agent, Codex can:

- Inspect your existing data stack and suggest:
  - Useful tools (`run_sql`, `get_errors_by_endpoint`, `fetch_kpi_timeseries`).
  - Safe query patterns and parameterization.
- Generate tool wrappers around:
  - Your database access layer.
  - Observability APIs (logs, traces, metrics).
  - Data transformation utilities.
- Help you design prompts that:
  - Emphasize using tools, not guessing.
  - Ask for clear explanations, not just numbers.
- Propose state models for your graph that include:
  - Questions, raw results, derived insights, and final answers.
- Generate tests that:
  - Use small sample datasets.
  - Check that the agent calls the right tools.
  - Validate that explanations mention key numbers and trends.

This turns Codex into a partner for building and evolving your internal data assistant.

---

## 8. Small diagram

High‑level architecture:

```text
Analyst / Dev → API / UI → Data Analysis Agent
                              ↓
                        [LLM + Tools]
                    ↙                     ↘
           [run_sql / metrics]      [CSV / file tools]
                              ↓
                    Intermediate insights & numbers
                              ↓
                         Final explanation
```

Logical flow:

```text
[Question]
   ↓
[Agent decides what data to fetch]
   ↓
[Call data tools → collect results]
   ↓
[Summarize insights + recommend next steps]
   ↓
[Reply]
```

---

## 9. Summary

- A data analysis agent is an AI assistant focused on reading, querying, and interpreting your data sources.
- You expose only the tools you trust for running queries, loading files, and fetching metrics.
- The agent uses these tools to gather numbers, then explains what they mean in everyday language.
- LangGraph manages the state and flow of analysis: question → tools → insights → summary.
- Codex can help you design tools, prompts, and tests so your data analysis agent becomes a safe, reliable part of your engineering and analytics workflow.


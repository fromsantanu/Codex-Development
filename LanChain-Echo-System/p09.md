# 9. Understanding Prompts for Agents

## 1. Simple Intro

When you work with AI agents, the **prompt** is just the **instruction you send to the model**.  
It’s like the **API request body** for the brain of your agent.

If the prompt is clear, the agent behaves well.  
If the prompt is messy, the agent behaves randomly.

In this chapter we’ll treat prompts like **well‑designed API contracts** that tell the agent what to do, what not to do, and how to answer.

---

## 2. Why Backend Devs Should Learn This

As a backend developer you already:

- Design routes and request/response formats.
- Validate input.
- Think about edge cases and safety.

Prompts are the **same game**, just with text instead of JSON:

- A good prompt = fewer bugs, fewer surprises.
- A reusable prompt = easier to maintain your agent.
- A testable prompt = easier to refactor with Codex.

Once you see prompts as **configurable behavior** for your agents, you can:

- Ship features faster (no new endpoint needed, just change the prompt).
- Control the style and tone of responses.
- Plug your prompts into LangChain chains, tools, and agents.

---

## 3. Analogy: Writing a Good Ticket for a Teammate

Imagine you open a Jira ticket for another developer:

- Bad ticket: *“Fix search.”*
- Good ticket: clear description, steps, examples, acceptance criteria.

The AI agent is like that teammate.  
The **prompt is the ticket**:

- If you say: *“Write code for this function.”* → it may guess wrong.
- If you say: *“You are a Python backend helper. Use SQLAlchemy. Follow PEP8. Return only code.”* → results are much better.

So: **Prompt = structured, detailed ticket for the AI.**

---

## 4. Key Ideas About Prompts

Here are the core pieces you’ll use with LangChain and Codex:

1. **System message**  
   - High‑level rules.  
   - Example: “You are a backend engineering assistant. Be concise. Return JSON.”

2. **User message**  
   - The actual request.  
   - Example: “Generate a FastAPI endpoint to create a user.”

3. **Few‑shot examples**  
   - Show the agent a couple of “input → ideal output” pairs.  
   - Like unit tests that live inside the prompt.

4. **Structured templates**  
   - Use placeholders for variables like `{user_question}` or `{schema}`.  
   - This makes prompts reusable in chains and agents.

5. **Guardrails in text**  
   - Say what NOT to do.  
   - Example: “If you don’t know, say you don’t know.”

6. **Prompt as code**  
   - Store prompts in Python files or templates.  
   - Version them in Git.  
   - Refactor them with Codex just like normal code.

---

## 5. Step‑by‑Step: Building a Good Agent Prompt

We’ll build a simple “backend helper” prompt step by step.

### Step 1: Define the Role

Think: *What kind of teammate is this agent?*

- “You are a senior Python backend engineer.”
- “You help write and improve APIs.”
- “You are careful with SQL.”

### Step 2: Define the Tasks

List **what it should do**:

- “Generate Python code when asked.”
- “Explain changes in simple language.”
- “Suggest tests.”

### Step 3: Define the Output Format

Decide what the response should look like:

- “Return only a Python code block.”
- Or: “Return JSON with fields `explanation` and `code`.”

### Step 4: Add a Few‑Shot Example

Give it a small example of a good answer:

- Question: “Create a `/health` endpoint.”  
- Answer: short FastAPI snippet.

Now the agent understands your style.

### Step 5: Turn It Into a Template

Convert everything into a prompt template with variables:

- `{language}`  
- `{framework}`  
- `{request}`

You can then reuse it in LangChain for different tasks.

### Step 6: Connect to an Agent

Use this template inside a LangChain **chain** or **agent**:

- The agent receives a user query.
- Fills the template.
- Sends it to the model (Codex or another LLM).
- Returns the answer to your API.

---

## 6. Short Code Examples

### Example 1: Simple Prompt Template in LangChain

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

llm = ChatOpenAI(model="gpt-4.1-mini")  # or a Codex-like model

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful Python backend assistant."),
    ("user", "Help with: {task_description}")
])

chain = prompt | llm | StrOutputParser()

result = chain.invoke({"task_description": "Create a simple FastAPI /ping endpoint"})
print(result)
```

Here:

- `system` sets the role and style.
- `user` passes the dynamic task.
- `chain.invoke(...)` is like calling a service with parameters.

### Example 2: Few‑Shot Prompt for API Design

```python
from langchain_core.prompts import ChatPromptTemplate

api_prompt = ChatPromptTemplate.from_messages([
    ("system",
     "You are a senior backend engineer. "
     "Write clean, minimal FastAPI endpoints."),

    # Example 1
    ("user", "Create a health check endpoint."),
    ("assistant", "```python\nfrom fastapi import APIRouter\n\nrouter = APIRouter()\n\n@router.get('/health')\nasync def health():\n    return {'status': 'ok'}\n```"),

    # Real request
    ("user", "Create an endpoint to list users with pagination.")
])
```

This gives the model **one example** of your preferred style, then asks for a new endpoint.

You can also turn this into a template by replacing the last `user` message with:

```python
("user", "{request}")
```

and then passing `{"request": "Create an endpoint to list users with pagination."}`.

---

## 7. How Codex Helps with Prompts

Codex is very good at:

- Turning your **informal idea** into a clean prompt template.
- Adding **few‑shot examples** based on your existing code.
- Refactoring a messy prompt into **sections** (Role / Task / Format / Examples).
- Generating multiple **variants** so you can A/B test.
- Embedding your prompt into **LangChain chains and tools**.

Typical things you can ask Codex:

- “Take this messy description and turn it into a reusable LangChain `ChatPromptTemplate`.”
- “Add 2 few‑shot examples showing the style I want.”
- “Rewrite this prompt so the model always responds with JSON only.”
- “Extract all hard‑coded strings in this prompt into `{variables}`.”

So you can treat prompt work just like normal backend refactoring:

- Start simple.
- Let Codex propose a cleaner structure.
- Iterate quickly.

---

## 8. Small Diagram

### Prompt Flow Inside an Agent

```text
User Request
    ↓
API / Backend
    ↓
Fill Prompt Template
    ↓
LLM / Codex Model
    ↓
Agent Logic (tools, DB)
    ↓
Response to User
```

### Prompt as a Reusable Component

```text
[Prompt Template]
      ↑
   used by
      ↑
[Chain] → [Agent] → [Tools / DB]
```

---

## 9. Summary

- A **prompt** is just a well‑written instruction, like a detailed Jira ticket or API contract for your AI agent.
- Use **system messages** for rules and role, **user messages** for the current request, and **few‑shot examples** to show ideal behavior.
- Treat prompts as **code**: template them, version them, and refactor them with Codex.
- LangChain makes prompts **reusable** by turning them into templates you can plug into chains and agents.
- With a bit of structure, prompts become a powerful way to **control agent behavior** without changing your backend code.


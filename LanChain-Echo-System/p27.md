# 27. Web Scraping & RAG Agent

## 1. Introduction

Most real questions need **fresh information**: docs, blogs, pricing pages, API references, or internal sites.  
An LLM on its own only knows what it was trained on; it doesn’t automatically know the latest version of your docs or your private knowledge.

In this chapter you’ll learn how to build a **Web Scraping & RAG (Retrieval‑Augmented Generation) agent** that:
- Reads web pages or internal docs.
- Stores useful text in a searchable index.
- Uses that index to answer questions with grounded, up‑to‑date information.

---

## 2. Why backend developers should care

- Many backend questions are doc‑driven:
  - “How do I call this external API?”
  - “What does this error code mean?”
  - “Which query parameters are required?”
- A Web + RAG agent can:
  - Sit in front of your docs and knowledge bases.
  - Let users ask natural‑language questions.
  - Answer by pulling from your actual documentation, not guesses.
- For you, this means:
  - Less time answering repeat questions.
  - A single, searchable entry point over all your docs.
  - A pattern you can reuse for internal wikis, runbooks, and playbooks.

---

## 3. Real-life analogy

Imagine you have a **research assistant** and a **library**:

- The assistant:
  - Visits websites.
  - Reads articles.
  - Takes notes on important parts.
- The library:
  - Organizes those notes by topic.
  - Lets you quickly find the most relevant notes for a question.

When you ask a question:
1. The assistant walks into the library.
2. Searches for the most relevant notes.
3. Reads them.
4. Writes a clear answer, pointing back to what they read.

This is exactly what a Web Scraping & RAG agent does:
- **Scraping** = gathering and cleaning text from the web.
- **Vector index** = the smart library.
- **RAG** = search the library, then answer using the found notes.

---

## 4. Key concepts

- **Web scraping / loading**
  - Fetching pages with HTTP.
  - Parsing HTML and extracting main content (not menus/ads).
  - Optionally reading local files (Markdown, HTML, PDF exports).

- **Document**
  - A piece of text with metadata:
    - `content` (the text chunk).
    - `source` (URL or file path).
    - `title`, `section`, or tags.

- **Chunking**
  - Splitting long documents into smaller pieces:
    - Example: 500–1000 characters with overlap.
  - Helps retrieval return focused, relevant chunks.

- **Embeddings**
  - Converting text into vectors (lists of numbers).
  - Similar meaning → similar vectors.
  - Used for semantic search (search by meaning, not exact words).

- **Vector store**
  - Database for embeddings + metadata.
  - Supports “give me the K documents closest to this query vector”.

- **Retrieval**
  - Take a user question.
  - Turn it into an embedding.
  - Find the most similar chunks in the vector store.

- **Generation**
  - Feed retrieved chunks + question into the LLM.
  - Ask it to answer **using only that context**.

- **RAG agent**
  - An agent that:
    - Has a tool to search the vector store.
    - Calls that tool first to get context.
    - Then generates an answer from the retrieved text.

---

## 5. Steps explained simply

1. **Choose your knowledge sources**
   - Public API docs.
   - Product and pricing pages.
   - Internal wiki / runbooks.

2. **Load and clean the content**
   - Use loaders (or your own scraping code) to:
     - Fetch pages or read files.
     - Strip boilerplate (navigation, ads, sidebars).
     - Keep titles, headings, and main text.

3. **Chunk and embed documents**
   - Split large pages into chunks (e.g. by paragraph or fixed size).
   - For each chunk:
     - Compute an embedding.
     - Store: `embedding`, `text`, and metadata (URL, section).
   - Save into a vector store (FAISS, Chroma, pgvector, etc.).

4. **Expose retrieval as a tool**
   - Write a function like:
     - `search_knowledge(query: str) -> str`
   - Inside:
     - Turn `query` into an embedding.
     - Run similarity search.
     - Return a compact string with top chunks + sources.

5. **Define the RAG agent behavior**
   - System prompt guidelines:
     - Always call the retrieval tool before answering.
     - Use only retrieved text as the factual basis.
     - If the docs don’t cover something, say you don’t know.

6. **Build a simple graph with LangGraph**
   - Example nodes:
     - `retrieve`: call the retrieval tool and store context in state.
     - `answer`: use LLM to answer from that context.
   - State might include:
     - `question`, `context`, `answer`.

7. **Connect it to your API / UI**
   - Backend endpoint:
     - Accepts `question`.
     - Invokes the graph with initial state.
     - Returns final answer (and optionally the sources).

8. **Refresh the index over time**
   - Scheduled job:
     - Re‑crawl or reload docs.
     - Re‑chunk and re‑embed changed pages.
   - Keeps answers synced with your documentation.

---

## 6. Short code examples

Below is a minimal RAG‑style workflow using a fake retrieval tool.  
In real applications you would plug in a real vector store and document pipeline.

```python
from typing import TypedDict
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END


# 1) Retrieval tool (stub; replace with real vector search)
@tool
def search_knowledge(query: str) -> str:
    """Search the knowledge base for relevant documentation."""
    # In real code, run a similarity search over your embedded docs.
    return (
        "Relevant docs:\n"
        "- /docs/api/orders: How to create and list orders.\n"
        "- /docs/auth: Using API keys and auth headers.\n"
    )


# 2) State for one RAG request
class RagState(TypedDict):
    question: str
    context: str
    answer: str


llm = ChatOpenAI(model="gpt-4.1-mini")


# 3) Retrieval node
def retrieve_node(state: RagState) -> RagState:
    runnable = llm.bind_tools([search_knowledge])

    system_prompt = (
        "You are a documentation assistant.\n"
        "Use the search_knowledge tool to find relevant docs "
        "before answering any question.\n"
    )

    result = runnable.invoke(
        [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": state["question"]},
        ]
    )

    context = str(result.content)

    return {
        **state,
        "context": context,
    }


# 4) Answer node
def answer_node(state: RagState) -> RagState:
    prompt = (
        "You answer questions using only the documentation below.\n\n"
        f"Question:\n{state['question']}\n\n"
        f"Docs:\n{state['context']}\n\n"
        "Give a clear, concise answer. "
        "If the docs do not contain the answer, say so."
    )
    result = llm.invoke(prompt)
    answer = str(result.content)

    return {
        **state,
        "answer": answer,
    }


# 5) Build the graph
workflow = StateGraph(RagState)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("answer", answer_node)

workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "answer")
workflow.add_edge("answer", END)

app = workflow.compile()


# 6) Backend helper
def ask_docs(question: str) -> str:
    state = app.invoke(
        {
            "question": question,
            "context": "",
            "answer": "",
        }
    )
    return state["answer"]
```

In real code you would:
- Implement `search_knowledge` to talk to your vector store.
- Add an offline script to:
  - Scrape/load pages.
  - Chunk and embed them.
  - Save embeddings and metadata.

---

## 7. How Codex helps

When building a Web Scraping & RAG agent, Codex can:

- Generate scrapers/loaders for:
  - Public docs sites.
  - Internal wikis and Markdown trees.
  - Mixed HTML/Markdown/PDF directories.
- Scaffold your RAG pipeline:
  - Chunking logic.
  - Embedding calls.
  - Vector store integration and retrieval tools.
- Help design prompts that:
  - Encourage grounded answers.
  - Avoid hallucinating facts outside the retrieved context.
  - Optionally include source links in answers.
- Produce indexing scripts and refresh jobs:
  - For re‑scraping and re‑embedding pages.
- Generate small test setups:
  - Insert a few fake docs.
  - Ask questions.
  - Assert that answers reference the right snippets.

This lets you focus on **which** docs to include and **how** you want them used, while Codex helps wire up the plumbing.

---

## 8. Small diagram

High‑level architecture:

```text
           ┌───────────────┐
Docs / Web│ Scraper +      │
─────────▶│ Index Builder  │
           └──────┬────────┘
                  │ embeddings + chunks
                  ↓
          [Vector Store / Index]
                  │
User → API → RAG Agent
                  ↓
           [Retrieve Context]
                  ↓
            [LLM Answer]
                  ↓
             Final Response
```

Logical flow:

```text
[User question]
      ↓
[Retrieve relevant chunks]
      ↓
[Generate answer from chunks]
      ↓
[Return answer (+ optional sources)]
```

---

## 9. Summary

- Web Scraping + RAG lets your system answer questions from **real, current documentation** instead of frozen model knowledge.
- Scraping/loaders turn pages into clean text; embeddings and vector stores make that text searchable by meaning.
- A RAG agent:
  - Retrieves relevant chunks for each question.
  - Then asks the LLM to explain or summarize them.
- LangGraph gives you a clear structure for this flow: question → retrieve → answer.
- Codex can help you build scrapers, indexing jobs, vector integrations, prompts, and tests so your Web + RAG agent becomes a dependable way to query your knowledge.

